{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Perceptron: Forward Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Perceptron it is a linear combination of inputs added by a bias passing throught a non-linear activation function defined as:\n",
    "\n",
    "$$\n",
    "\\hat{y} = g\\Biggl(\\sum_{i=1}^{m} x_i w_i + w_0 \\Biggr)\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $x_i$ represents the input from $i$ to $m$;\n",
    "    \n",
    "- $w_i$ represents the weights from $i$ to $m$;\n",
    "    \n",
    "- $w_0$ represents the bias for each neuron;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be implemented as follow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Pytorch\n",
    "try:\n",
    "    import torch\n",
    "except:\n",
    "    !pip3 install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron(torch.nn.Module):\n",
    "    def __init__(self, x_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Initialize weights and bias randomlly\n",
    "        self.w = torch.randn(x_dim, 1)\n",
    "        self.b = torch.zeros(1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        z = torch.matmul(x, self.w) + self.b  # Equivalent to xT*w + b\n",
    "        g = torch.sigmoid(z)\n",
    "        \n",
    "        return g\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Curiosity about building standard models with PyTorch:: [init and forward method](https://discuss.pytorch.org/t/beginner-should-relu-sigmoid-be-called-in-the-init-method/18689/5)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neuron output: 0.95\n"
     ]
    }
   ],
   "source": [
    "b_dim = 1  # Number of samples\n",
    "x_dim = 20  # Feature dimension\n",
    "\n",
    "x = torch.randn(b_dim, x_dim) # Equivalent to xT in matrix notation\n",
    "\n",
    "model = Perceptron(x.shape[-1]) # Auto-detected the input dimension, same as x_dim\n",
    "\n",
    "y = model(x)\n",
    "\n",
    "print(f\"Neuron output: {y.item():.02}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Perceptrons can be used to create single layers defined as:\n",
    "\n",
    "Hidden:\n",
    "$$\n",
    "z_i = \\sum_{j=1}^{m} x_{j} w_{ji}^{(1)} + w_{0,i}^{(1)}\n",
    "$$\n",
    "\n",
    "Output:\n",
    "$$\n",
    "\\hat{y} = g\\Biggl(\\sum_{j=1}^{d1} g(z_{j}) w_{ji}^{(2)} + w_{0,i}^{(2)} \\Biggr)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Layers (deep)\n",
    "\n",
    "Dense neural networks can be created stacking multiple layers:\n",
    "\n",
    "Hidden:\n",
    "$$\n",
    "z_{i}^{(k)} = \\sum_{j=1}^{n^{k-1}} g(z_{j})^{(k-1)} w_{j,i}^{(k)} + w_{0,i}^{(k)}\n",
    "$$\n",
    "\n",
    "Output:\n",
    "$$\n",
    "\\hat{y_i} = g\\Biggl(\\sum_{j=1}^{n^{k-1}} g(z_{j})^{(k-1)} w_{j,i}^{(k)} + w_{0,i}^{k)} \\Biggr)\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $g_(z_{j})^{(k-1)}$: represents the outputs of each neuron $j$ from the previous layer $k-1$;\n",
    "\n",
    "- $w_{j,i}$: represents the weights for each neuron $j$ from previous layer and the output for each neuron $i$ from current layer $k$;\n",
    "    \n",
    "- $w_{0,i} $ represents the bias for each neuron $i$ from current layer $k$;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, x_dim, h_units):\n",
    "        super().__init__()\n",
    "        # Layers\n",
    "        self.linear_hidden = torch.nn.Linear(x_dim, h_units)\n",
    "        self.linear_output = torch.nn.Linear(h_units, 1)\n",
    "        \n",
    "        # Activations\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # hidden layer\n",
    "        zh = self.linear_hidden(x)\n",
    "        gh = self.relu(zh)\n",
    "        \n",
    "        # output layer\n",
    "        zo = self.linear_output(gh)\n",
    "        go = self.sigmoid(zo)\n",
    "        \n",
    "        return go"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5607]], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "h_units = 8\n",
    "\n",
    "# Model configuration: x_dim:h_dim:o_dim -> 20:8:8:4 \n",
    "model = MLP(x.shape[-1], h_units)\n",
    "\n",
    "y = model(x)\n",
    "\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantify prediction\n",
    "\n",
    "To quantify incorrect predictions it can be used a loss (objective, cost or empirical risk) function.\n",
    "\n",
    "The loss function quantifies how well or poorly your model is performing on the given task.\n",
    "\n",
    "It measures the error or discrepancy between the predicted output of your model and the actual target values.\n",
    "\n",
    "$$J(W) = \\frac{1}{N} \\sum_{i=1}^{N} \\mathcal{L}(f(x^{(i)};W), y^{(i)})$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $f(x^{(i)};W)$: represents the model prediction for each output $i$;\n",
    "- $y^{(i)}$: represents the desired output for each output $i$;\n",
    "\n",
    "As example, the Mean Square Error (MSE) loss can be defined as:\n",
    "\n",
    "$$MSE = \\frac{1}{N} \\sum_{i=1}^{N} (\\hat{y}_i - y_{i})^2$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss optimization\n",
    "Predictions can be modified based on some optmization algorithm. \n",
    "\n",
    "For neural network it can be defined as:\n",
    "\n",
    "$$W^* = \\arg\\min_{W} \\frac{1}{N} \\sum_{i=1}^{N} \\mathcal{L}(f(x^{(i)};W), y^{(i)})$$\n",
    "\n",
    "\n",
    "The formula represents the general loss optimization for neural networks:\n",
    "\n",
    "- $W$: The parameters of the neural network that are being optimized to minimize the loss.\n",
    "- $\\mathcal{L}(.)$: The loss function that quantifies the discrepancy between the predicted outputs and the target outputs.\n",
    "- $N$: The total number of data samples in the dataset.\n",
    "- $x_i$: The input features of the $i$-th data sample.\n",
    "- $f(x^{(i)};W)$: The prediction made by the neural network for input $x_i$ using parameters $W$.\n",
    "- $y_i$: The target output for the $i$-th data sample.\n",
    "- $\\mathcal{L}(f(x^{(i)};W)$: A per-sample loss function that measures the difference between the predicted output and the target output.\n",
    "\n",
    "The goal of the optimization is to minimize $\\mathcal{L}(.)$ by adjusting the parameters $W$ of the neural network.\n",
    "\n",
    "The most famous methods are the gradient based ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient\n",
    "\n",
    "The gradient of a function with respect to its parameters represents the direction and magnitude of the steepest descent or ascent in the function at a particular point. In other words, is a vector that points in the direction of the maximum (or minimum) rate of decrease or increase of the function.\n",
    "\n",
    "In this optimization case, computation of loss, the aim is to find the values of the model's parameters that minimize the loss function.\n",
    "\n",
    "Gradient Descent (Algorithm)\n",
    "\n",
    "1. Randomly initialize parameters\n",
    "2. Loop until convergence:\n",
    "3. &emsp; Compute gradient of the loss (backpropagation)\n",
    "4. &emsp; Update weights based on gradient\n",
    "5. Return weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation\n",
    "\n",
    "The process of computing gradients for all the model's parameters is typically performed using an algorithm called backpropagation. \n",
    "\n",
    "Backpropagation efficiently computes the gradients of the loss function with respect to each parameter by applying the chain rule of calculus.\n",
    "\n",
    "The best way to understand backpropagation I believe is doing a simple example.\n",
    "\n",
    "The example [2] used here will be calculating the gradient of the equation bellow:\n",
    "$$ r = w^2 $$\n",
    "\n",
    "where\n",
    "$$w = zv$$\n",
    "$$v = u+y$$\n",
    "$$u = x^2$$\n",
    "\n",
    "thus\n",
    "$$ r = z^2 (x^2 + y)^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating local gradients (partial derivatives):\n",
    "\n",
    "1. Partial derivative of $r$ with respect to $w$:\n",
    "$\n",
    "\\frac{\\partial r}{\\partial w} = \\frac{\\partial w^2}{\\partial w} = 2w\n",
    "$\n",
    "\n",
    "2. Partial derivative of $w$ with respect to $z$ and $v$:\n",
    "$\n",
    "\\frac{\\partial w}{\\partial z} = \\frac{\\partial zv}{\\partial z} = v\n",
    "\\frac{\\partial w}{\\partial v} = \\frac{\\partial zv}{\\partial v} = z\n",
    "$\n",
    "\n",
    "3. Partial derivative of $v$ with respect to $u$ and $y$:\n",
    "$\n",
    "\\frac{\\partial v}{\\partial u} = \\frac{\\partial u+y}{\\partial u} = 1\n",
    "\\frac{\\partial v}{\\partial y} = \\frac{\\partial u+y}{\\partial y} = 1\n",
    "$\n",
    "\n",
    "4. Partial derivative of $u$ with respect to $x$:\n",
    "$\n",
    "\\frac{\\partial u}{\\partial x} = \\frac{\\partial x^2}{\\partial x} = 2x\n",
    "$\n",
    "\n",
    "5. Partial derivative of $r$ with respect to $z$:\n",
    "$\n",
    "\\frac{\\partial r}{\\partial z} = \\frac{\\partial r}{\\partial w} \\frac{\\partial w}{\\partial z} = 2wv\n",
    "$\n",
    "\n",
    "6. Partial derivative of $r$ with respect to $y$:\n",
    "$\n",
    "\\frac{\\partial r}{\\partial y} = \\frac{\\partial r}{\\partial w} \\frac{\\partial w}{\\partial v} \\frac{\\partial v}{\\partial y} = 2wz\n",
    "$\n",
    "\n",
    "7. Partial derivative of $r$ with respect to $x$:\n",
    "$\n",
    "\\frac{\\partial r}{\\partial x} = \\frac{\\partial r}{\\partial w} \\frac{\\partial w}{\\partial v} \\frac{\\partial v}{\\partial u} \\frac{\\partial u}{\\partial x}= 2wz2x\n",
    "$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward: 144\n",
      "Gradients: drdz=72, drdy=96, drdx=192\n"
     ]
    }
   ],
   "source": [
    "# input\n",
    "x = 1; y = 2; z = 4\n",
    "\n",
    "# nodes\n",
    "u = x**2\n",
    "v = u+y\n",
    "w = z*v\n",
    "\n",
    "# forward\n",
    "r = w**2\n",
    "print(f\"Forward: {r}\")\n",
    "\n",
    "# backward\n",
    "drdz = 2*w*v\n",
    "drdy = 2*w*z\n",
    "drdx = 2*w*z*2*x\n",
    "\n",
    "print(f\"Gradients: drdz={drdz}, drdy={drdy}, drdx={drdx}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For small problems calculating the gradient can be simple, but for large neural models calculating the partial derivative of each node and applying chain rules can be challenging and trick.\n",
    "\n",
    "This is where frameworks like Pytorch shine, they can do this automatically like in pytorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Pytorch ([autograd](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r = 144.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dr/dx = 192.0\n",
      "dr/dy = 96.0\n",
      "dr/dz = 72.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.tensor(1.0, requires_grad=True)\n",
    "y = torch.tensor(2.0, requires_grad=True)\n",
    "z = torch.tensor(4.0, requires_grad=True)\n",
    "\n",
    "# forward pass\n",
    "u = x**2\n",
    "v = u+y\n",
    "w = z*v\n",
    "r = w**2\n",
    "print(f'r = {r}')\n",
    "\n",
    "# backward pass\n",
    "r.backward()  # simple like this\n",
    "\n",
    "print(f'dr/dx = {x.grad}')\n",
    "print(f'dr/dy = {y.grad}')\n",
    "print(f'dr/dz = {z.grad}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main pieces:\n",
    "- Loss: Tell us how good or bad predictions are compared with target.\n",
    "- Gradient: How weights should be changed to improve loss, in the case decrease loss (negative gradient)\n",
    "- Updating weitghs: Change weights based on previous values, loss and gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting everything togeter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,     1] loss: 0.107\n",
      "[1,     2] loss: 0.108\n",
      "[1,     3] loss: 0.121\n",
      "[1,     4] loss: 0.124\n",
      "[1,     5] loss: 0.199\n",
      "[1,     6] loss: 0.322\n",
      "[1,     7] loss: 0.530\n",
      "[1,     8] loss: 0.571\n",
      "[1,     9] loss: 0.608\n",
      "[1,    10] loss: 0.738\n",
      "[2,     1] loss: 0.088\n",
      "[2,     2] loss: 0.092\n",
      "[2,     3] loss: 0.096\n",
      "[2,     4] loss: 0.096\n",
      "[2,     5] loss: 0.140\n",
      "[2,     6] loss: 0.222\n",
      "[2,     7] loss: 0.371\n",
      "[2,     8] loss: 0.388\n",
      "[2,     9] loss: 0.447\n",
      "[2,    10] loss: 0.535\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "# Random input data just for example\n",
    "n_samples = 10\n",
    "\n",
    "data = torch.rand(n_samples, x_dim)\n",
    "labels = torch.rand(n_samples, 1)\n",
    "\n",
    "samples = list(zip(data, labels))\n",
    "\n",
    "epochs = 2\n",
    "\n",
    "# Optmize loss\n",
    "optim = torch.optim.SGD(model.parameters(), lr=1e-2, momentum=0.9)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    i = 0\n",
    "    for feature, target in samples:\n",
    "        # zero the parameter gradients\n",
    "        optim.zero_grad()\n",
    "        \n",
    "        # forward\n",
    "        predictions = model(feature)\n",
    "\n",
    "        # calculate loss\n",
    "        loss = torch.nn.MSELoss()(predictions, target)\n",
    "\n",
    "        # Compute gradient\n",
    "        loss.backward()\n",
    "\n",
    "        # Update\n",
    "        optim.step()\n",
    "        \n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss:.3f}')\n",
    "        \n",
    "        i += 1\n",
    "        \n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code doesn't learn nothing given random data and labels, is just an illustration example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "\n",
    "[Lecture 1 - Intro to Deep Learning](https://www.youtube.com/watch?v=QDX-1M5Nj7s&list=PLtBw6njQRU-rwp5__7C0oIVt26ZgjG9NI&index=3)\n",
    "\n",
    "[Backpropagation - Chain Rile and Pytorch in action](https://towardsdatascience.com/backpropagation-chain-rule-and-pytorch-in-action-f3fb9dda3a7d)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
