{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autoregressive large language models are build on top of transformers architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer is a neural network architecture based on self-attention mechanisms, avoiding thus recurrence \n",
    "connections. This way sequence problems can be solved in parallel, making training large models faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Self attention mechanisms relies on compute similarity between embeddings. For NLP they can create relations between words helps a given task such as translation.\n",
    "\n",
    "I think self attention mechanisms works as learning to read without knowing grammar. For example, \"The boy kicked the ball and it disappered.\" Attention layer can process \"it\" while at same time\n",
    "is looking for other words. With time, I believe the layer will start to perceive that when objects appears in a sentece \"it\" will be appearing to replace or determine the object itself. This is what a pronoum does. If this happens without be parallel, it will be very dificult to perceive this relations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand in detail the attention mechanisms The Attention is All You Need paper [2] is a good place to\n",
    "understand the transformer architecture.\n",
    "\n",
    "I create my personal finds and placed on this repositoy https://github.com/ramonlins/obsidian/tree/master/Papers/transformers/Ashish%20Vaswani.\n",
    "\n",
    "To access the material is necessary to install obisidian and excalidraw plugin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In attempt to create a simple example to implement transforms from scratch I created a simple \n",
    "overview of the architecture abstracting some parts such as residual connections, stack of layers and multiple heads.\n",
    "\n",
    "![transformer](./images/transformer.png \"Figure 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea here is to create a squence model to predict the next character vowel based on a sequence of three characters extracted from the portugues alphabet. \n",
    "\n",
    "\n",
    "In this simpler ilustration the encoder is composed by a attention layer connected with a feed forward network; \n",
    "Its outputs are connected with another attention layer togeter with the output of the masked attention layer; This attention layer output is connected to another feedforward layer that pass through a linear and\n",
    "softmax operation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input sequence is first embedded into word vectors using an embedding layer. These word vectors serve as inputs for both query and key transformations.\n",
    "\n",
    "Separate linear transformations with learned weight matrices are then applied to these word embeddings to obtain query vectors (Q) and key vectors (K). The purpose of these separate transformations is to project each word's embedding into lower-dimensional spaces that capture different aspects of their meaning.\n",
    "\n",
    "SQ and K embeddings differ because they undergo distinct linear transformations with unique weight matrices during computation.\n",
    "\n",
    "In the Transformer model, during training, the Q (query) matrix learns to search for relevant information based on what K (keys) are showing as reference.\n",
    "\n",
    "The similarities are highlighted passing trough softmax working like amplifiers to the synaptic connections for a given input and task in matrix V (the data itself).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding next vowel\n",
    "pt-alphabet is defined by:\n",
    "\n",
    "\"abcdefghijklmnopqrstuvwxyz\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = \"-abcdefghijklmnopqrstuvwxyz#\"\n",
    "emb = {}\n",
    "pos = {}\n",
    "for i, c in enumerate(x):\n",
    "    pos[c] = i         # position i mapping\n",
    "    #emb[c] = format(i,'05b') # binary encoding mapping        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create all possible combinations of input and ouputs\n",
    "combinations = \"---abcdefghijklmnopqrstuvwxyz#\"\n",
    "vowels = 'aeoiu#'\n",
    "n_seq = 3\n",
    "n = len(x)\n",
    "\n",
    "vowel_index = {}\n",
    "for i, v in enumerate(combinations):\n",
    "    if v in vowels:\n",
    "        vowel_index[v] = i\n",
    "        \n",
    "samples = []\n",
    "dim_alphapet = len(combinations)\n",
    "for i in range(dim_alphapet-2):\n",
    "    input_seq = []\n",
    "    for j in range(n_seq):\n",
    "       input_seq.append(pos[combinations[i+j]])\n",
    "    #input_seq = combinations[i:i+3] \n",
    "    for vowel, idx in vowel_index.items():\n",
    "        if idx > i+2:\n",
    "            #next_vowel = vowel\n",
    "            next_vowel = pos[vowel]\n",
    "            break\n",
    "        \n",
    "        if i+2 >= 23:\n",
    "            next_vowel = pos['#']\n",
    "            break\n",
    "        \n",
    "    samples.append([input_seq, next_vowel])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "\n",
    "class Encoder:\n",
    "    def __init__(self, n, dm, h):\n",
    "        self.h = h                  # num of attention heads\n",
    "        self.dm = dm                # model dim\n",
    "        self.dk = dm//h\n",
    "        self.dv = 2                 # model dim over num of attention heads\n",
    "        \n",
    "        self.embedding_layer = torch.nn.Embedding(n, dm)  # n x dm\n",
    "        \n",
    "        self.wQ = torch.nn.Linear(dm, self.dk)  # dm x dk\n",
    "        self.wK = torch.nn.Linear(dm, self.dk)  # dm x dk\n",
    "        self.wV = torch.nn.Linear(dm, self.dv)  # dm x dv\n",
    "        self.ff1 = torch.nn.Linear(self.dv, dm*4)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.ff2 = torch.nn.Linear(dm*4, dm)\n",
    "        self.fc = torch.nn.Linear(3*dm, 6)\n",
    "        #self.ff3 = torch.nn.Linear(1, 6)\n",
    "        \n",
    "    def embedding(self, x):\n",
    "        return self.embedding_layer(x) #(b x t x n) x (n x dm) -> (b x t x dm)\n",
    "       \n",
    "    def attention(self, x_seq):\n",
    "        # Word embedding\n",
    "        pos_emb = self.embedding(x_seq)\n",
    "        \n",
    "        # Search\n",
    "        Q = self.wQ(pos_emb)  # (b x t x dm) x (dm x dk) -> (b x t x dk)\n",
    "        # Reference\n",
    "        K = self.wK(pos_emb)  # (b x t x dm) x (dm x dk) -> (b x t x dk)\n",
    "        # Attention features\n",
    "        V = self.wV(pos_emb)  # (b x t x dm) x (dm x dv) -> (b x t x dv)\n",
    "        \n",
    "        # Similarity\n",
    "        QK = torch.matmul(Q, K.T)  # (t x dk) x (dk x t) -> (t x t) *removing batch info\n",
    "        QKn = QK / torch.sqrt(torch.tensor([self.dm]))\n",
    "        \n",
    "        # Highlight similar words\n",
    "        P = torch.nn.Softmax(dim=0)(QKn)    # (t x t)\n",
    "        \n",
    "        # Focus on specific features\n",
    "        H = torch.matmul(P, V)              # (t x t) x (t x dv) -> (t x dv)\n",
    "        \n",
    "        # Attention latent space\n",
    "        R = self.ff1(H)  # (t x dv) x (dv x dm*4) -> (t x dm*4)\n",
    "        A = self.ff2(R)  # (t x dm*4) x (dm*4 x dm) -> (t x dm)\n",
    "        \n",
    "        # Probabilities\n",
    "        flatten_a = A.view(-1)                  # 1 x (t.dm))\n",
    "        logits = self.fc(flatten_a)             # t.dm x num of labels)\n",
    "        p = torch.nn.Softmax(dim=0)(logits)     # prob of each label\n",
    "        \n",
    "        return p\n",
    "    \n",
    "# class Decoder(Encoder):\n",
    "#     def __init__(self, n, dm, h):\n",
    "#         super().__init__(n, dm, h)\n",
    "        \n",
    "#     def attention(self):\n",
    "#         pass\n",
    "        \n",
    "#     def mask_attention():\n",
    "#         pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class transformer:\n",
    "    def __init__(self, n, dm, h):\n",
    "        self.y_seq = pos[random.choice(vowels)]\n",
    "        \n",
    "        self.encoder = Encoder(n=n, dm=dm, h=h)\n",
    "        #self.decoder = Decoder()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        a = self.encoder.attention(x)\n",
    "        \n",
    "        #de_e = self.decoder.embedding(self.y_seq)\n",
    "           \n",
    "        #h = self.decoder.mask_attention(self.y_seq)\n",
    "        \n",
    "        #p   = self.decoder.attention(a, h)\n",
    "            \n",
    "        #self.y_seq = pos[vowels[max(enumerate(p), key=lambda x: x[1])]]\n",
    "                \n",
    "        return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = transformer(n=n, dm=16, h=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = samples[0]\n",
    "x_seq = torch.tensor(data[0])\n",
    "x_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = transformer.forward(x_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1180, 0.1877, 0.2365, 0.1751, 0.1524, 0.1302],\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'o'"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vowels[max(enumerate(y), key=lambda x: x[1])[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training only with attention layer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
