{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autoregressive large language models are build on top of transformers architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer is a neural network architecture based on self-attention mechanisms, avoiding thus recurrence \n",
    "connections. This way sequence problems can be solved in parallel, making training large models faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Self attention mechanisms relies on compute similarity between embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand in detail the attention mechanisms The Attention is All You Need paper [2] is a good place to\n",
    "understand the transformer architecture.\n",
    "\n",
    "I create my personal finds and placed on this repositoy https://github.com/ramonlins/obsidian/tree/master/Papers/transformers/Ashish%20Vaswani.\n",
    "\n",
    "To access the material is necessary to install obisidian and excalidraw plugin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In attempt to create a simple example to implement transforms from scratch I created a simple \n",
    "overview of the architecture abstracting some parts such as residual connections, stack of layers and \n",
    "multiple heads.\n",
    "\n",
    "![transformer](./images/transformer.png \"Figure 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea here is to create a squence model to predict the next character vowel based on a sequence of three\n",
    "characters extracted from the portugues alphabet. \n",
    "\n",
    "\n",
    "In this simpler ilustration the encoder is composed by a attention layer connected with a feed forward network; \n",
    "Its outputs are connected with another attention layer togeter with the output of the masked attention \n",
    "layer; This attention layer output is connected to another feedforward layer that pass through a linear and\n",
    "softmax operation.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding next vowel\n",
    "pt-alphabet is defined by:\n",
    "\n",
    "\"abcdefghijklmnopqrstuvwxyz\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = \"-abcdefghijklmnopqrstuvwxyz\"\n",
    "emb = {}\n",
    "pos = {}\n",
    "for i, c in enumerate(x):\n",
    "    pos[c] = str(i)               # position i\n",
    "    emb[c] = format(i,'05b') # binary encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'-': '0',\n",
       " 'a': '1',\n",
       " 'b': '2',\n",
       " 'c': '3',\n",
       " 'd': '4',\n",
       " 'e': '5',\n",
       " 'f': '6',\n",
       " 'g': '7',\n",
       " 'h': '8',\n",
       " 'i': '9',\n",
       " 'j': '10',\n",
       " 'k': '11',\n",
       " 'l': '12',\n",
       " 'm': '13',\n",
       " 'n': '14',\n",
       " 'o': '15',\n",
       " 'p': '16',\n",
       " 'q': '17',\n",
       " 'r': '18',\n",
       " 's': '19',\n",
       " 't': '20',\n",
       " 'u': '21',\n",
       " 'v': '22',\n",
       " 'w': '23',\n",
       " 'x': '24',\n",
       " 'y': '25',\n",
       " 'z': '26'}"
      ]
     },
     "execution_count": 606,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'-': '00000',\n",
       " 'a': '00001',\n",
       " 'b': '00010',\n",
       " 'c': '00011',\n",
       " 'd': '00100',\n",
       " 'e': '00101',\n",
       " 'f': '00110',\n",
       " 'g': '00111',\n",
       " 'h': '01000',\n",
       " 'i': '01001',\n",
       " 'j': '01010',\n",
       " 'k': '01011',\n",
       " 'l': '01100',\n",
       " 'm': '01101',\n",
       " 'n': '01110',\n",
       " 'o': '01111',\n",
       " 'p': '10000',\n",
       " 'q': '10001',\n",
       " 'r': '10010',\n",
       " 's': '10011',\n",
       " 't': '10100',\n",
       " 'u': '10101',\n",
       " 'v': '10110',\n",
       " 'w': '10111',\n",
       " 'x': '11000',\n",
       " 'y': '11001',\n",
       " 'z': '11010'}"
      ]
     },
     "execution_count": 607,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create all possible combinations of input and ouputs\n",
    "combinations = \"---abcdefghijklmnopqrstuvwxyz\"\n",
    "vowels = 'aeoiu'\n",
    "\n",
    "vowel_index = {}\n",
    "for i, v in enumerate(combinations):\n",
    "    if v in vowels:\n",
    "        vowel_index[v] = i\n",
    "        \n",
    "samples = []\n",
    "dim_alphapet = len(combinations)\n",
    "for i in range(dim_alphapet-2):\n",
    "    input_seq = combinations[i:i+3]\n",
    "    for vowel, idx in vowel_index.items():\n",
    "        if idx > i+2:\n",
    "            next_vowel = vowel\n",
    "            break\n",
    "        \n",
    "        if i+2 >= 23:\n",
    "            next_vowel = '#'\n",
    "            break\n",
    "        \n",
    "    samples.append([input_seq, next_vowel])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder:\n",
    "    def __init__(self, x_dim, dm, h):\n",
    "        #h = h\n",
    "        #dm = dm\n",
    "        #dk = dv = dm/h\n",
    "        #de = x_dim + 1\n",
    "        \n",
    "        #Q = torch.nn.Linear(de, dk)\n",
    "        #K = torch.nn.Linear(de, dk)\n",
    "        #V = torch.nn.Linear(de, dv)\n",
    "        pass\n",
    "    \n",
    "    def embedding(self, c):\n",
    "        # get char encode\n",
    "        ec = emb[c] + pos[c]\n",
    "\n",
    "        return ec\n",
    "    \n",
    "    def attention(self):\n",
    "        self.matmul()\n",
    "        self.scale()\n",
    "        self.softmax()\n",
    "        self.matmul()\n",
    "        \n",
    "        attention = 0\n",
    "        \n",
    "        return attention\n",
    "              \n",
    "    def matmul(self):\n",
    "        print('matmul')\n",
    "    \n",
    "    def scale(self):\n",
    "        print('scale')\n",
    "        \n",
    "    def softmax(self):\n",
    "        print('softmax')\n",
    "    \n",
    "    \n",
    "class Decoder(Encoder):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def embedding(self, _y):\n",
    "        ed = emb[_y] + pos[_y] \n",
    "        led = [int(c) for c in ed]\n",
    "        emb_dec_t = torch.tensor(led).float().requires_grad_(True)\n",
    "        ed_pos = int(pos[_y])\n",
    "        \n",
    "        return emb_dec_t, ed_pos\n",
    "    \n",
    "    def mask_attention(self, \n",
    "                       attention, \n",
    "                       emb,\n",
    "                       ec_pos,\n",
    "                       ed_pos):\n",
    "        self.matmul()\n",
    "        self.scale()\n",
    "        self.mask()\n",
    "        \n",
    "        # mask\n",
    "        if ed_pos <= max(ec_pos):\n",
    "            print(\"Position of emb dec is less than all idx in seq, masking..\")\n",
    "            mask_tensor = torch.zeros_like(emb)\n",
    "            mask_tensor[:] = float('inf')\n",
    "            \n",
    "            o = -masked_tensor\n",
    "        else:\n",
    "            print(\"Position of emb dec is bigger than all idx in seq\")\n",
    "            \n",
    "            o = emb\n",
    "        self.softmax()\n",
    "        self.matmul()\n",
    "        \n",
    "        return o\n",
    "   \n",
    "    def mask(self):\n",
    "        print(\"mask\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 613,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class transformer:\n",
    "    def __init__(self):\n",
    "        self._y = 'a'\n",
    "        self.encoder = Encoder(x_dim=3, dm=8, h=1)\n",
    "        self.decoder = Decoder()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        ec = ''\n",
    "        ec_pos = []\n",
    "        # encoding embedding\n",
    "        for c in x:\n",
    "            ec += self.encoder.embedding(c)\n",
    "            ec_pos.append(int(pos[c]))\n",
    "                        \n",
    "        # encoder\n",
    "        print(\"Encoder\")\n",
    "        attention = self.encoder.attention()\n",
    "        print(f\"Encoder attention {attention} \\n\")\n",
    "        \n",
    "        ## Decoder\n",
    "        print(\"Decoder\")\n",
    "        emb_dec_t, ed_pos = self.decoder.embedding(self._y)\n",
    "        \n",
    "        y = self.decoder.mask_attention(attention, \n",
    "                                        emb_dec_t,\n",
    "                                        ec_pos,\n",
    "                                        ed_pos)\n",
    "        \n",
    "        print(f\"Decoder mask output {y}\")\n",
    "        \n",
    "        self.decoder.attention()\n",
    "        \n",
    "        self._y = y\n",
    "          \n",
    "        #return y\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 614,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = transformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder\n",
      "matmul\n",
      "scale\n",
      "softmax\n",
      "matmul\n",
      "Encoder attention 0 \n",
      "\n",
      "Decoder\n",
      "matmul\n",
      "scale\n",
      "mask\n",
      "Position of emb dec is less than all idx in seq, masking..\n",
      "softmax\n",
      "matmul\n",
      "Decoder mask output tensor([-inf, -inf, -inf, -inf, -inf, -inf], grad_fn=<NegBackward0>)\n",
      "matmul\n",
      "scale\n",
      "softmax\n",
      "matmul\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([-inf, -inf, -inf, -inf, -inf, -inf], grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 615,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = 'abc'\n",
    "y = transformer.forward(x)\n",
    "y"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
