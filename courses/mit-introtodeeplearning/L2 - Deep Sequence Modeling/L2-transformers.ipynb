{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Jupyter Notebook explores the basic concepts of transformers and their application using a simple case example to predict the next vowel of portuguese alphabet based on a sequence of characters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Transformers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autoregressive large language models are built on top of the transformer architecture, which is a neural network architecture based on self-attention mechanisms. Transformers are designed to handle sequence data efficiently in parallel, making training large models faster. Self-attention mechanisms are crucial for natural language processing (NLP) tasks as they create relationships between words, aiding tasks like translation.\n",
    "\n",
    "Self-attention mechanisms can be thought of as learning to read without knowing grammar. For example, in the sentence \"The boy kicked the ball and it disappeared,\" an attention layer can process \"it\" while simultaneously looking for other words. Over time, the layer can learn to associate the appearance of objects in a sentence with the usage of \"it,\" similar to how pronouns work.\n",
    "\n",
    "To understand attention mechanisms in detail, refer to the \"Attention is All You Need\" paper [2]. I create my personal finds and placed on this repositoy [Paper Notes](https://github.com/ramonlins/obsidian/tree/master/Papers/transformers/Ashish%20Vaswani)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In attempt to create a simple example to implement a transformer from scratch I created a simple overview of the architecture abstracting some parts such as residual connections, stack of layers and multiple heads, as shown in figure 1. \n",
    "\n",
    "The idea here is to create a sequence model to predict the next character vowel based on a sequence of three characters extracted from the Portuguese alphabet. \n",
    "\n",
    "![transformer](./images/transformer.png \"Figure 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this illustration, the encoder is composed of an attention layer connected with a feed-forward network; Its outputs are connected with the decoder attention layer together with the output of the masked attention layer; This attention layer output is connected to another feedforward layer passing through a linear and softmax operations.\n",
    "\n",
    "The input sequence is first embedded into word vectors using an embedding layer. These word vectors serve as inputs for both query and key transformations.\n",
    "\n",
    "The linear transformations are then applied to these word embeddings to obtain query vectors (Q) and key vectors (K). The purpose of these transformations is to project each word's embedding into lower-dimensional spaces to capture different aspects of their meaning.\n",
    "\n",
    "The Q and K-V pair embeddings differ because they undergo distinct linear transformations with unique weight matrices during computation. Hence, during training, the Q (query) matrix learns to search for relevant information based on what K (keys) are showing as reference.\n",
    "\n",
    "The softmax operation, applied within attention mechanisms, serves to highlight similarities by amplifying relevant connections between queries and keys weights. This normalization process helps compute attention scores that emphasize important information in the data (V).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding next vowel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "\n",
    "pt-alphabet is defined by: \"abcdefghijklmnopqrstuvwxyz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "characters = \"-abcdefghijklmnopqrstuvwxyz#\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ascending character (index) arragement\n",
    "def generate_combinations(characters, length):\n",
    "    # Traverse the characters recursively\n",
    "    def generate_combinations_recursive(current_combination, remaining_characters):\n",
    "        if len(current_combination) == length:\n",
    "            combinations.append(current_combination)\n",
    "            return\n",
    "        for i in range(len(remaining_characters)):\n",
    "            generate_combinations_recursive(\n",
    "                current_combination + remaining_characters[i],\n",
    "                remaining_characters[i + 1:]  # Get next character in sequence\n",
    "            )\n",
    "\n",
    "    combinations = []\n",
    "    generate_combinations_recursive('', characters)\n",
    "    return combinations\n",
    "\n",
    "# Input sequence length\n",
    "n_seq = 3\n",
    "\n",
    "# Generate all possible ordered combinations\n",
    "combinations = generate_combinations(characters, n_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mapping characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define output vowels with a special end character\n",
    "vowels = 'aeiou#'\n",
    "\n",
    "# Size of alphabet\n",
    "n = len(characters)\n",
    "emb = {}\n",
    "c2i, i2c, v2i = {}, {}, {}\n",
    "\n",
    "for i, c in enumerate(characters):\n",
    "    c2i[c] = i  # mapping character to index position\n",
    "    i2c[i] = c  # mapping index position to character\n",
    "    if c in vowels:\n",
    "        v2i[c] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'a', 1: 'e', 2: 'i', 3: 'o', 4: 'u', 5: '#'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i2v = {}\n",
    "for i, v in enumerate(vowels):\n",
    "   i2v[i] = v  # mapping vowels to label position\n",
    "i2v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize and create input-target pair data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Create all possible combinations of input and outputs\n",
    "valid_seqs = []       # Store all valid sequences\n",
    "samples = []          # Store samples (input, target)\n",
    "\n",
    "# Tokkenize data based on index\n",
    "for combination in combinations:\n",
    "    character_idx = c2i[combination[-1]]  # Get the position of the last character in the combination\n",
    "    input_seq = []  # Store the input sequence\n",
    "    # Iterate through each vowel and its position in alphabet\n",
    "    for vowel, vowel_idx in v2i.items():\n",
    "        # Check which vowel is the next one in the alphabet\n",
    "        if vowel_idx > character_idx:\n",
    "            #input_seq = []  # Store the input sequence\n",
    "            \n",
    "            # Convert characters to its position (tokenization)\n",
    "            for c in combination:\n",
    "                input_seq.append(c2i[c])\n",
    "            \n",
    "            valid_seqs.append(input_seq)  # Add the input sequence to valid sequences\n",
    "            \n",
    "            # Convert the tokenized input sequence and next vowel index to tensors\n",
    "            input_seq_t = torch.tensor(input_seq)\n",
    "            next_vowel_t = torch.tensor([vowel_idx])\n",
    "    \n",
    "            samples.append([input_seq_t, next_vowel_t])  # Create input-target pair sample\n",
    "            \n",
    "            break  # Break out of the loop once a valid vowel is found\n",
    "        \n",
    "        # Check if the character index is greater than the index of 'u'\n",
    "        if character_idx > c2i['u']:\n",
    "            #input_seq = []  # Create an empty list to store the input sequence\n",
    "            \n",
    "            # Iterate through each character in the combination\n",
    "            for c in combination:\n",
    "                input_seq.append(c2i[c])  # Append the index of the character to the input sequence\n",
    "            \n",
    "            valid_seqs.append(input_seq)  # Add the input sequence to the list of valid sequences\n",
    "            \n",
    "            # Convert the input sequence and target index tensors\n",
    "            input_seq_t = torch.tensor(input_seq)\n",
    "            next_vowel_t = torch.tensor([c2i['#']])  # Define a special character for letters bigger than 'u'\n",
    "    \n",
    "            samples.append([input_seq_t, next_vowel_t])\n",
    "            \n",
    "            break # Break out of the loop once the there is no more valid vowel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shuffle and split dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "seed = 13\n",
    "\n",
    "X = [sample[0] for sample in samples]\n",
    "y = [sample[1] for sample in samples]\n",
    "\n",
    "X = shuffle(X, random_state=seed)\n",
    "y = shuffle(y, random_state=seed)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    An encoder module for a transformer-based neural network.\n",
    "\n",
    "    This encoder class processes input sequences using one-head self-attention and feedforward layers.\n",
    "\n",
    "    Args:\n",
    "        n (int): The size of the input vocabulary.\n",
    "        dm (int): The model's embedding dimension.\n",
    "    \n",
    "    Attributes:\n",
    "        dm (int): The model dimension.\n",
    "        dk (int): The dimension of keys.\n",
    "        dv (int): The dimension of values.\n",
    "        embedding_layer (torch.nn.Embedding): The input embedding layer.\n",
    "        wQ (torch.nn.Linear): Linear layer for queries in attention.\n",
    "        wK (torch.nn.Linear): Linear layer for keys in attention.\n",
    "        wV (torch.nn.Linear): Linear layer for values in attention.\n",
    "        norm1 (torch.nn.LayerNorm): Layer normalization after attention.\n",
    "        relu (torch.nn.ReLU): ReLU activation function.\n",
    "        ff1 (torch.nn.Linear): First feedforward layer.\n",
    "        ff2 (torch.nn.Linear): Second feedforward layer.\n",
    "        norm2 (torch.nn.LayerNorm): Layer normalization after feedforward layers.\n",
    "\n",
    "    Methods:\n",
    "        embedding(x): Maps input indices to embeddings.\n",
    "        attention(x_seq): Performs one-head self-attention on input sequences.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, n, dm):\n",
    "        super().__init__()\n",
    "        self.dm = dm  # model dim\n",
    "        self.dk = dm \n",
    "        self.dv = dm  \n",
    "        self.embedding_layer = torch.nn.Embedding(n, dm)  # n x dm\n",
    "         \n",
    "        self.wQ = torch.nn.Linear(dm, self.dk)  # dm x dk\n",
    "        self.wK = torch.nn.Linear(dm, self.dk)  # dm x dk\n",
    "        self.wV = torch.nn.Linear(dm, self.dv)  # dm x dv\n",
    "        self.norm1 = torch.nn.LayerNorm(dm)\n",
    "        \n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.ff1 = torch.nn.Linear(self.dv, dm) # dv x dm\n",
    "        self.ff2 = torch.nn.Linear(dm, dm)      # dm x dm\n",
    "        self.norm2 = torch.nn.LayerNorm(dm)\n",
    "        \n",
    "    def embedding(self, x):\n",
    "        \"\"\"\n",
    "        Maps input indices to embeddings.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input indices of shape (batch_size=1, sequence_length).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Embeddings of shape (batch_size=1, sequence_length, model_dimension).\n",
    "        \"\"\"\n",
    "        return self.embedding_layer(x) #(t x n) x (n x dm) -> (t x dm)\n",
    "       \n",
    "    def attention(self, x_seq):\n",
    "        \"\"\"\n",
    "        Performs multi-head self-attention on input sequences.\n",
    "\n",
    "        Args:\n",
    "            x_seq (torch.Tensor): Input sequences of shape (batch_size=1, sequence_length, model_dimension).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output of the attention layer of shape (batch_size=1, sequence_length, model_dimension).\n",
    "        \"\"\"\n",
    "        # Convert to word embedding\n",
    "        pos_emb = self.embedding(x_seq)\n",
    "        \n",
    "        # Search\n",
    "        Q = self.wQ(pos_emb)  # (t x dm) x (dm x dk) -> (t x dk)\n",
    "        # Reference\n",
    "        K = self.wK(pos_emb)  # (t x dm) x (dm x dk) -> (t x dk)\n",
    "        # Attention features\n",
    "        V = self.wV(pos_emb)  # (t x dm) x (dm x dv) -> (t x dv)\n",
    "        \n",
    "        # Similarity\n",
    "        QK = torch.matmul(Q, K.T)  # (t x dk) x (dk x t) -> (t x t)\n",
    "        QKn = QK / torch.sqrt(torch.tensor([self.dm]))\n",
    "        \n",
    "        # Highlight similar words\n",
    "        P = torch.nn.Softmax(dim=-1)(QKn)  # (t x t)\n",
    "        \n",
    "        # Focus on specific features\n",
    "        H = torch.matmul(P, V)  # (t x t) x (t x dv) -> (t x dv)\n",
    "        Hn = self.norm1(H)\n",
    "        \n",
    "        # Attention latent space\n",
    "        R = self.relu(self.ff1(Hn))  # (t x dv) x (dv x dm) -> (t x dm)\n",
    "        A = self.ff2(R)  # (t x dm) x (dm x dm) -> (t x dm)\n",
    "        An = self.norm2(A)\n",
    "        \n",
    "        return An\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class Decoder(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    A decoder module for a transformer-based neural network.\n",
    "\n",
    "    This decoder class processes input sequences using one-head self-attention and feedforward layers, \n",
    "    and generates output sequences.\n",
    "\n",
    "    Args:\n",
    "        n (int): The size of the output vocabulary.\n",
    "        dm (int): The model's embedding dimension.\n",
    "\n",
    "    Attributes:\n",
    "        _y (int): An internal variable to store the index of the generated output.\n",
    "        dm (int): The model dimension.\n",
    "        dk (int): The dimension of keys.\n",
    "        dv (int): The dimension of values.\n",
    "        embedding_layer (torch.nn.Embedding): The output embedding layer.\n",
    "        norm1 (torch.nn.LayerNorm): Layer normalization after attention.\n",
    "        wQ (torch.nn.Linear): Linear layer for queries in attention.\n",
    "        wK (torch.nn.Linear): Linear layer for keys in attention.\n",
    "        wV (torch.nn.Linear): Linear layer for values in attention.\n",
    "        norm2 (torch.nn.LayerNorm): Layer normalization after feedforward layers.\n",
    "        relu (torch.nn.ReLU): ReLU activation function.\n",
    "        ff1 (torch.nn.Linear): First feedforward layer.\n",
    "        ff2 (torch.nn.Linear): Second feedforward layer.\n",
    "        norm3 (torch.nn.LayerNorm): Layer normalization after the final feedforward layer.\n",
    "        fc (torch.nn.Linear): Linear layer for generating output logits.\n",
    "\n",
    "    Methods:\n",
    "        embedding(x): Maps output indices to embeddings.\n",
    "        attention(ae, ad): Performs one-head self-attention on input sequences.\n",
    "        masked_attention(x_seq): Performs masked one-head self-attention on input sequences.\n",
    "    \"\"\"\n",
    "    def __init__(self, n, dm):\n",
    "        super().__init__()\n",
    "        self._y = random.randint(0, 5)  # Start from any vowel character\n",
    "        self.dm = dm  # model dim\n",
    "        self.dk = dm \n",
    "        self.dv = dm \n",
    "        \n",
    "        self.embedding_layer = torch.nn.Embedding(n, dm)  # n x dm\n",
    "        self.norm1 = torch.nn.LayerNorm(dm)\n",
    "        \n",
    "        self.wQ = torch.nn.Linear(dm, self.dk)  # dm x dk\n",
    "        self.wK = torch.nn.Linear(dm, self.dk)  # dm x dk\n",
    "        self.wV = torch.nn.Linear(dm, self.dv)  # dm x dv\n",
    "        self.norm2 = torch.nn.LayerNorm(dm)\n",
    "        \n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.ff1 = torch.nn.Linear(self.dv, dm)\n",
    "        self.ff2 = torch.nn.Linear(dm, dm)\n",
    "        self.norm3 = torch.nn.LayerNorm(dm)\n",
    "        self.fc = torch.nn.Linear(n_seq*dm, len(vowels))\n",
    "        \n",
    "    def embedding(self, x):\n",
    "        \"\"\"\n",
    "        Maps output indices to embeddings.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Output indices of shape (batch_size=1, sequence_length).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Embeddings of shape (batch_size=1, sequence_length, model_dimension).\n",
    "        \"\"\"\n",
    "        return self.embedding_layer(x) #(b x t x n) x (n x dm) -> (b x t x dm)\n",
    "    \n",
    "    def attention(self, ae, ad):\n",
    "        \"\"\"\n",
    "        Performs multi-head self-attention on input sequences.\n",
    "\n",
    "        Args:\n",
    "            ae (torch.Tensor): Input sequences of shape (batch_size=1, sequence_length, model_dimension) for reference.\n",
    "            ad (torch.Tensor): Input sequences of shape (batch_size=1, sequence_length, model_dimension) for search.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output of the attention layer of shape (batch_size=1, sequence_length, model_dimension).\n",
    "        \"\"\"\n",
    "        # Search\n",
    "        Q = self.wQ(ad)  # (t x dm) x (dm x dk) -> (t x dk)\n",
    "        # Reference\n",
    "        K = self.wK(ae)  # (t x dm) x (dm x dk) -> (t x dk)\n",
    "        # Attention features\n",
    "        V = self.wV(ae)  # (t x dm) x (dm x dv) -> (t x dv)\n",
    "        \n",
    "        # Similarity\n",
    "        QK = torch.matmul(Q, K.T)  # (t x dk) x (dk x t) -> (t x t)\n",
    "        QKn = QK / torch.sqrt(torch.tensor([self.dm]))\n",
    "        \n",
    "        # Highlight similar words\n",
    "        P = torch.nn.Softmax(dim=-1)(QKn)    # (t x t)\n",
    "        \n",
    "        # Focus on specific features\n",
    "        H = torch.matmul(P, V)              # (t x t) x (t x dv) -> (t x dv)\n",
    "        Hn = self.norm2(H)\n",
    "        \n",
    "        # Attention latent space\n",
    "        R = self.relu(self.ff1(Hn))  # (t x dv) x (dv x dm*4) -> (t x dm*4)\n",
    "        A = self.ff2(R)  # (t x dm) x (dm x dm) -> (t x dm)\n",
    "        An = self.norm3(A)\n",
    "        \n",
    "        # Probabilities\n",
    "        flatten_a = An.view(-1)                  # 1 x (t x dm)\n",
    "        logits = self.fc(flatten_a)             # (t.dm) x num of labels\n",
    "        p = torch.nn.Softmax(dim=0)(logits)     # prob of each label\n",
    "        \n",
    "        # Get token id of output vowel\n",
    "        vowel_idx = max(enumerate(p), key=lambda t: t[1])[0]\n",
    "        vowel = vowels[vowel_idx]\n",
    "        self._y = c2i[vowel]\n",
    "        \n",
    "        return p\n",
    "    \n",
    "    def masked_attention(self, x_seq):\n",
    "        \"\"\"\n",
    "        Performs masked multi-head self-attention on input sequences.\n",
    "\n",
    "        Args:\n",
    "            x_seq (torch.Tensor): Input sequences of shape (batch_size, sequence_length, model_dimension).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output of the masked attention layer of shape (batch_size, sequence_length, model_dimension).\n",
    "        \"\"\"\n",
    "        # shift output embedding\n",
    "        x_seq = x_seq.tolist()\n",
    "        x_seq.pop(-1)\n",
    "        x_seq.insert(0, self._y)\n",
    "        _x_seq = torch.tensor(x_seq)\n",
    "        \n",
    "        # Word embedding\n",
    "        pos_emb = self.embedding(_x_seq)\n",
    "        \n",
    "        # Search\n",
    "        Q = self.wQ(pos_emb)  # (b x t x dm) x (dm x dk) -> (b x t x dk)\n",
    "        # Reference\n",
    "        K = self.wK(pos_emb)  # (b x t x dm) x (dm x dk) -> (b x t x dk)\n",
    "        # Attention features\n",
    "        V = self.wV(pos_emb)  # (b x t x dm) x (dm x dv) -> (b x t x dv)\n",
    "        \n",
    "        \n",
    "        # Similarity\n",
    "        QK = torch.matmul(Q, K.T)  # (t x dk) x (dk x t) -> (t x t) *removing batch info\n",
    "        \n",
    "        # Mask\n",
    "        if x_seq not in valid_seqs:\n",
    "            mask = torch.tensor([[0, 0, 0],\n",
    "                                 [1, 1, 1],\n",
    "                                 [1, 1, 1],])\n",
    "        \n",
    "            QK.masked_fill_(mask == 0, -1e9)\n",
    "        \n",
    "        QKn = QK / torch.sqrt(torch.tensor([self.dm]))\n",
    "        \n",
    "        # Highlight similar words\n",
    "        P = torch.nn.Softmax(dim=0)(QKn)    # (t x t)\n",
    "        \n",
    "        # Focus on specific features\n",
    "        H = torch.matmul(P, V)              # (t x t) x (t x dv) -> (t x dv)\n",
    "        \n",
    "        # Attention latent space\n",
    "        R = self.relu(self.ff1(H))  # (t x dv) x (dv x dm*4) -> (t x dm*4)\n",
    "        A = self.ff2(R)  # (t x dm*4) x (dm*4 x dm) -> (t x dm)\n",
    "        An = self.norm1(H)\n",
    "        \n",
    "        return An\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class transformer(torch.nn.Module):\n",
    "    def __init__(self, n, dm):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(n=n, dm=dm)\n",
    "        self.decoder = Decoder(n=n, dm=dm)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # encoder\n",
    "        ae = self.encoder.attention(x)\n",
    "        # decoder\n",
    "        ad = self.decoder.masked_attention(x)\n",
    "        y = self.decoder.attention(ae, ad)\n",
    "            \n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformer(\n",
      "  (encoder): Encoder(\n",
      "    (embedding_layer): Embedding(28, 16)\n",
      "    (wQ): Linear(in_features=16, out_features=16, bias=True)\n",
      "    (wK): Linear(in_features=16, out_features=16, bias=True)\n",
      "    (wV): Linear(in_features=16, out_features=16, bias=True)\n",
      "    (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "    (relu): ReLU()\n",
      "    (ff1): Linear(in_features=16, out_features=16, bias=True)\n",
      "    (ff2): Linear(in_features=16, out_features=16, bias=True)\n",
      "    (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (embedding_layer): Embedding(28, 16)\n",
      "    (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "    (wQ): Linear(in_features=16, out_features=16, bias=True)\n",
      "    (wK): Linear(in_features=16, out_features=16, bias=True)\n",
      "    (wV): Linear(in_features=16, out_features=16, bias=True)\n",
      "    (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "    (relu): ReLU()\n",
      "    (ff1): Linear(in_features=16, out_features=16, bias=True)\n",
      "    (ff2): Linear(in_features=16, out_features=16, bias=True)\n",
      "    (norm3): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "    (fc): Linear(in_features=48, out_features=6, bias=True)\n",
      "  )\n",
      ")\n",
      "Epoch: 1 \tTraining Loss: 1.4525\n",
      "Epoch: 2 \tTraining Loss: 1.4463\n",
      "Epoch: 3 \tTraining Loss: 1.4462\n",
      "Epoch: 4 \tTraining Loss: 1.3995\n",
      "Epoch: 5 \tTraining Loss: 1.3464\n",
      "Epoch: 6 \tTraining Loss: 1.2793\n",
      "Epoch: 7 \tTraining Loss: 1.2223\n",
      "Epoch: 8 \tTraining Loss: 1.1995\n",
      "Epoch: 9 \tTraining Loss: 1.1892\n",
      "Epoch: 10 \tTraining Loss: 1.1850\n",
      "Epoch: 11 \tTraining Loss: 1.1827\n",
      "Epoch: 12 \tTraining Loss: 1.1807\n",
      "Epoch: 13 \tTraining Loss: 1.1791\n",
      "Epoch: 14 \tTraining Loss: 1.1782\n",
      "Epoch: 15 \tTraining Loss: 1.1775\n",
      "Epoch: 16 \tTraining Loss: 1.1626\n",
      "Epoch: 17 \tTraining Loss: 1.0827\n",
      "Epoch: 18 \tTraining Loss: 1.0715\n",
      "Epoch: 19 \tTraining Loss: 1.0723\n",
      "Epoch: 20 \tTraining Loss: 1.0711\n",
      "Changing learning rate to 0.0020833333333333333\n",
      "Epoch: 21 \tTraining Loss: 1.0704\n",
      "Epoch: 22 \tTraining Loss: 1.0705\n",
      "Epoch: 23 \tTraining Loss: 1.0701\n",
      "Epoch: 24 \tTraining Loss: 1.0700\n",
      "Epoch: 25 \tTraining Loss: 1.0701\n",
      "Epoch: 26 \tTraining Loss: 1.0705\n",
      "Epoch: 27 \tTraining Loss: 1.0700\n",
      "Epoch: 28 \tTraining Loss: 1.0702\n",
      "Epoch: 29 \tTraining Loss: 1.0703\n",
      "Epoch: 30 \tTraining Loss: 1.0699\n",
      "Epoch: 31 \tTraining Loss: 1.0702\n",
      "Epoch: 32 \tTraining Loss: 1.0698\n",
      "Epoch: 33 \tTraining Loss: 1.0698\n",
      "Epoch: 34 \tTraining Loss: 1.0697\n",
      "Epoch: 35 \tTraining Loss: 1.0697\n",
      "Epoch: 36 \tTraining Loss: 1.0697\n",
      "Epoch: 37 \tTraining Loss: 1.0697\n",
      "Epoch: 38 \tTraining Loss: 1.0696\n",
      "Epoch: 39 \tTraining Loss: 1.0696\n",
      "Epoch: 40 \tTraining Loss: 1.0695\n",
      "Changing learning rate to 0.0017361111111111112\n",
      "Epoch: 41 \tTraining Loss: 1.0696\n",
      "Epoch: 42 \tTraining Loss: 1.0695\n",
      "Epoch: 43 \tTraining Loss: 1.0694\n",
      "Epoch: 44 \tTraining Loss: 1.0694\n",
      "Epoch: 45 \tTraining Loss: 1.0694\n",
      "Epoch: 46 \tTraining Loss: 1.0697\n",
      "Epoch: 47 \tTraining Loss: 1.0694\n",
      "Epoch: 48 \tTraining Loss: 1.0693\n",
      "Epoch: 49 \tTraining Loss: 1.0693\n",
      "Epoch: 50 \tTraining Loss: 1.0690\n",
      "Epoch: 51 \tTraining Loss: 1.0591\n",
      "Epoch: 52 \tTraining Loss: 1.0519\n",
      "Epoch: 53 \tTraining Loss: 1.0499\n",
      "Epoch: 54 \tTraining Loss: 1.0484\n",
      "Epoch: 55 \tTraining Loss: 1.0483\n",
      "Epoch: 56 \tTraining Loss: 1.0478\n",
      "Epoch: 57 \tTraining Loss: 1.0477\n",
      "Epoch: 58 \tTraining Loss: 1.0476\n",
      "Epoch: 59 \tTraining Loss: 1.0477\n",
      "Epoch: 60 \tTraining Loss: 1.0476\n",
      "Changing learning rate to 0.0014467592592592594\n",
      "Epoch: 61 \tTraining Loss: 1.0476\n",
      "Epoch: 62 \tTraining Loss: 1.0476\n",
      "Epoch: 63 \tTraining Loss: 1.0476\n",
      "Epoch: 64 \tTraining Loss: 1.0476\n",
      "Epoch: 65 \tTraining Loss: 1.0476\n",
      "Epoch: 66 \tTraining Loss: 1.0476\n",
      "Epoch: 67 \tTraining Loss: 1.0476\n",
      "Epoch: 68 \tTraining Loss: 1.0475\n",
      "Epoch: 69 \tTraining Loss: 1.0475\n",
      "Epoch: 70 \tTraining Loss: 1.0475\n",
      "Epoch: 71 \tTraining Loss: 1.0475\n",
      "Epoch: 72 \tTraining Loss: 1.0475\n",
      "Epoch: 73 \tTraining Loss: 1.0475\n",
      "Epoch: 74 \tTraining Loss: 1.0475\n",
      "Epoch: 75 \tTraining Loss: 1.0475\n",
      "Epoch: 76 \tTraining Loss: 1.0475\n",
      "Epoch: 77 \tTraining Loss: 1.0475\n",
      "Epoch: 78 \tTraining Loss: 1.0475\n",
      "Epoch: 79 \tTraining Loss: 1.0475\n",
      "Epoch: 80 \tTraining Loss: 1.0475\n",
      "Changing learning rate to 0.0012056327160493829\n",
      "Epoch: 81 \tTraining Loss: 1.0475\n",
      "Epoch: 82 \tTraining Loss: 1.0475\n",
      "Epoch: 83 \tTraining Loss: 1.0475\n",
      "Epoch: 84 \tTraining Loss: 1.0475\n",
      "Epoch: 85 \tTraining Loss: 1.0475\n",
      "Epoch: 86 \tTraining Loss: 1.0475\n",
      "Epoch: 87 \tTraining Loss: 1.0475\n",
      "Epoch: 88 \tTraining Loss: 1.0475\n",
      "Epoch: 89 \tTraining Loss: 1.0475\n",
      "Epoch: 90 \tTraining Loss: 1.0475\n",
      "Epoch: 91 \tTraining Loss: 1.0475\n",
      "Epoch: 92 \tTraining Loss: 1.0475\n",
      "Epoch: 93 \tTraining Loss: 1.0475\n",
      "Epoch: 94 \tTraining Loss: 1.0475\n",
      "Epoch: 95 \tTraining Loss: 1.0475\n",
      "Epoch: 96 \tTraining Loss: 1.0475\n",
      "Epoch: 97 \tTraining Loss: 1.0475\n",
      "Epoch: 98 \tTraining Loss: 1.0475\n",
      "Epoch: 99 \tTraining Loss: 1.0475\n",
      "Epoch: 100 \tTraining Loss: 1.0474\n",
      "Finished training\n"
     ]
    }
   ],
   "source": [
    "# Encode targets\n",
    "def one_hot_encode(t):\n",
    "    one_hot_encode = torch.zeros(len(vowels))  # Create one hot encode buffer\n",
    "    \n",
    "    for i, v in enumerate(vowels):\n",
    "        # Check if the current vowel 'v' matches the target vowel 't'\n",
    "        if v == t:\n",
    "            one_hot_encode[i] = 1  # Set the corresponding index to 1 if there's a match\n",
    "    \n",
    "    # Convert the one-hot tensor to a float tensor and enable gradient tracking\n",
    "    return one_hot_encode.float().requires_grad_(True)\n",
    "\n",
    "# Get number of samples\n",
    "train_samples = list(zip(X_train, y_train))\n",
    "test_samples = list(zip(X_test, y_test))\n",
    "\n",
    "n_samples = len(train_samples)\n",
    "\n",
    "# Build model\n",
    "model = transformer(n=n, dm=16)\n",
    "\n",
    "print(model)\n",
    "\n",
    "# Define optmization algorithm\n",
    "opt = torch.optim.SGD(model.parameters(), lr=0.0025, momentum=0.9)\n",
    "\n",
    "# Define loss\n",
    "cel = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    rl = 0.0\n",
    "    for x_seq, t in train_samples:\n",
    "        # avoid cumulative gradient\n",
    "        opt.zero_grad()\n",
    "        \n",
    "        # predictions\n",
    "        p = model(x_seq)\n",
    "\n",
    "        # compute error\n",
    "        vowel = characters[t.item()]\n",
    "        t_enc = one_hot_encode(vowel)\n",
    "        loss = cel(p, t_enc)\n",
    "        \n",
    "        # compute gradient\n",
    "        loss.backward()\n",
    "        \n",
    "        # Clip the gradients\n",
    "        max_grad_norm = 1.0  # Set your desired maximum gradient norm here\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "\n",
    "        # update weights\n",
    "        opt.step()\n",
    "        \n",
    "        # running loss\n",
    "        rl += loss.item()\n",
    "    \n",
    "    # Handcraft learnig rate decay\n",
    "    if epoch % (0.2*epochs) == 0 and epoch > 0:\n",
    "       for param_group in opt.param_groups:\n",
    "           print(f\"Changing learning rate to {param_group['lr']/1.2}\")\n",
    "           param_group['lr'] = param_group['lr']/1.2\n",
    "    \n",
    "    # avg running loss\n",
    "    rl /= n_samples\n",
    "    \n",
    "    print(f\"Epoch: {epoch+1} \\tTraining Loss: {rl:.4f}\")\n",
    "    \n",
    "print(\"Finished training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model, samples):\n",
    "    c = 0  # Initialize a counter for correct predictions\n",
    "    \n",
    "    for sample in samples:\n",
    "        p = model(sample[0])\n",
    "        \n",
    "        # Find the index of the highest predicted value in 'p'\n",
    "        i = max(enumerate(p), key=lambda x: x[1])[0]\n",
    "        \n",
    "        # Map the index 'i' to the corresponding vowel using 'i2v'\n",
    "        prediction = i2v[i]\n",
    "        \n",
    "        # Map the target index (sample[1]) to the corresponding character using 'i2c'\n",
    "        target = i2c[sample[1].item()]\n",
    "        \n",
    "        # Check if the model's prediction matches the target character\n",
    "        if prediction == target:\n",
    "            c += 1  # Increment the correct prediction counter\n",
    "    \n",
    "    # Calculate and print the percentage of correct predictions\n",
    "    accuracy_percentage = (c / len(samples)) * 100\n",
    "    print(f\"Accuracy %: {accuracy_percentage}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy %: 99.89827060020346\n"
     ]
    }
   ],
   "source": [
    "# target x\n",
    "eval(model, test_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like learning has happened.... \n",
    "\n",
    "I stopped here because the purpose of this task was just to understand transformer using a simple example. \n",
    " \n",
    "More complex models can be build using transformers layers such as [TransformerEncoder](https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoderLayer.html#torch.nn.TransformerEncoderLayer), [TransformerDecoderLayer](https://pytorch.org/docs/stable/generated/torch.nn.TransformerDecoderLayer.html) from pytorch or other framework.\n",
    "\n",
    "It also possible to use pre build models [bert](https://huggingface.co/docs/transformers/model_doc/bert), \n",
    "[llama2](https://huggingface.co/docs/transformers/model_doc/llama2) among [others](https://huggingface.co/docs/transformers/index).\n",
    "\n",
    "The next step will be focus on understanding the fundamentals of generative networks with simple examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference:\n",
    "1. https://www.youtube.com/watch?v=ySEx_Bqxvvo&list=PLtBw6njQRU-rwp5__7C0oIVt26ZgjG9NI&index=2\n",
    "2. https://github.com/ramonlins/obsidian/tree/master/Papers/transformers/Ashish%20Vaswani\n",
    "3. https://github.com/ramonlins/obsidian/blob/master/Papers/transformers/Ashish%20Vaswani/Attention%20Is%20All%20You%20Need.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
