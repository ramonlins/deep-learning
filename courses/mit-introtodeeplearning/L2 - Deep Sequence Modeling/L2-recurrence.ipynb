{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple concept idea\n",
    "Recurrent neural networks models are linked to time and sequencial problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhaps the most simple recurrent network is the Elman Neural Network [2] illustrated in Figure 1.\n",
    "\n",
    "![Elman Net](./images/srn2.png \"Figure 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Elman Net can be denoted as Simple Recurrent Network (SRN) and can be described mathematically [2] as:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "x(t) & =w(t)+s(t-1) \\\\\n",
    "s_j(t) & =f\\left(\\sum_i x_i(t) u_{j i}\\right) \\\\\n",
    "y_k(t) & =g\\left(\\sum_j s_j(t) v_{k j}\\right)\n",
    "\\end{aligned}\n",
    "$$\n",
    "The sigmoid activation function:\n",
    "$$\n",
    "f(z)=\\frac{1}{1+e^{-z}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where:\n",
    "\n",
    "- $x(t)$ is the input at time $t$ formed by concatenating:\n",
    "    - $w(t)$ represents the current word;\n",
    "    - $s(t-1)$ is the output from neurons in context layer $s$ at time $t-1$;\n",
    "- $s_j(t)$ is the context output at time $t$ with $j$ (#hidden neurons);\n",
    "- $y_k(t)$ is the output at time $t$ with $k$ (output dim);\n",
    "- $u_{ij}$ represents the input weights with $i$ (input dim);\n",
    "- $v_{kj}$ represents the output weights bias;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think this representation is very similar to the representation shown in [mit-lecture2-RNNs](https://www.youtube.com/watch?v=ySEx_Bqxvvo&list=PLtBw6njQRU-rwp5__7C0oIVt26ZgjG9NI&index=3) with recurrent cell and unfold illustration through time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In an attempt to understand basic fundamentals of recurrent nets, I created a network from scratch to try to solve the xor problem as a sequence as proposed by Elman [3]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The sequence XOR Problem\n",
    "Input : 1010001110101 ...\n",
    "\n",
    "Output: 011100100111? ...\n",
    "\n",
    "When the network has received the first bit-1 in the example above-there is a 50% chance that the next bit will be a 1 (or a 0). When the network receives the second bit (0),\n",
    "however, it should then be possible to predict that the third will be the XOR, 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Reproducibility\n",
    "torch.manual_seed(13)\n",
    "\n",
    "# Create output XOR sequence\n",
    "def xor(w):\n",
    "    # The first element of sequence has 50% of chance of happens\n",
    "    y = torch.randint(0, 2, (1,))\n",
    "    \n",
    "    # The next elements consider two time steps\n",
    "    for i in range(len(w)-1):\n",
    "        y = torch.cat((y, w[i] ^ w[i+1]), 0) # xor sequence\n",
    "\n",
    "    # Add batch dimension \n",
    "    y = y.view(y.shape[0], 1)\n",
    "    \n",
    "    return y\n",
    "\n",
    "# Define random sequence input\n",
    "n_samples = 300\n",
    "w = torch.randint(0, 2, (n_samples, 1,))\n",
    "\n",
    "# Get target output\n",
    "l = xor(w)\n",
    "\n",
    "# Concatenate the data\n",
    "samples = list(zip(w.float(), l.float()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For debug, uncomment to track wich node autograd has problem \n",
    "#torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "class SRN(torch.nn.Module):\n",
    "    def __init__(self, w_dim, h_dim):\n",
    "        super().__init__()\n",
    "        self.hx = torch.nn.Linear(w_dim + h_dim, h_dim)  # Hidden layer with input (w) + context units (s-1)        \n",
    "        self.ho = torch.nn.Linear(h_dim, 1)\n",
    "        self.s_ = torch.rand(h_dim) # Random initialize context units, hence: \n",
    "                                    # for t=0 and h_dim=2 -> s(t-1) = [s0(-1), s1(-1)]\n",
    "        # Activation function\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, w): \n",
    "        e = torch.cat((w, self.s_)) # w(t) + s(t-1)\n",
    "        s = self.sigmoid(self.hx(e)) # s(t)\n",
    "        o = self.ho(s)               # logits\n",
    "        y = self.sigmoid(o)         # probs\n",
    "        \n",
    "        self.s_ = s.detach()        # old context\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SRN(\n",
      "  (hx): Linear(in_features=5, out_features=4, bias=True)\n",
      "  (ho): Linear(in_features=4, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n",
      "Epoch: 1 \tTraining Loss: 0.754951\n",
      "Epoch: 2 \tTraining Loss: 0.741379\n",
      "Epoch: 3 \tTraining Loss: 0.737758\n",
      "Epoch: 4 \tTraining Loss: 0.732614\n",
      "Epoch: 5 \tTraining Loss: 0.717725\n",
      "Epoch: 6 \tTraining Loss: 0.687708\n",
      "Epoch: 7 \tTraining Loss: 0.673383\n",
      "Epoch: 8 \tTraining Loss: 0.662864\n",
      "Epoch: 9 \tTraining Loss: 0.608036\n",
      "Epoch: 10 \tTraining Loss: 0.550075\n",
      "Epoch: 11 \tTraining Loss: 0.464484\n",
      "Epoch: 12 \tTraining Loss: 0.449802\n",
      "Epoch: 13 \tTraining Loss: 0.465556\n",
      "Epoch: 14 \tTraining Loss: 0.403875\n",
      "Epoch: 15 \tTraining Loss: 0.490509\n",
      "Epoch: 16 \tTraining Loss: 0.457623\n",
      "Epoch: 17 \tTraining Loss: 0.446882\n",
      "Epoch: 18 \tTraining Loss: 0.435092\n",
      "Epoch: 19 \tTraining Loss: 0.436977\n",
      "Epoch: 20 \tTraining Loss: 0.437063\n",
      "Changing learning rate to 0.05\n",
      "Epoch: 21 \tTraining Loss: 0.411269\n",
      "Epoch: 22 \tTraining Loss: 0.484933\n",
      "Epoch: 23 \tTraining Loss: 0.441611\n",
      "Epoch: 24 \tTraining Loss: 0.356973\n",
      "Epoch: 25 \tTraining Loss: 0.234155\n",
      "Epoch: 26 \tTraining Loss: 0.235428\n",
      "Epoch: 27 \tTraining Loss: 0.240688\n",
      "Epoch: 28 \tTraining Loss: 0.241087\n",
      "Epoch: 29 \tTraining Loss: 0.236761\n",
      "Epoch: 30 \tTraining Loss: 0.233993\n",
      "Epoch: 31 \tTraining Loss: 0.255466\n",
      "Epoch: 32 \tTraining Loss: 0.300173\n",
      "Epoch: 33 \tTraining Loss: 0.234077\n",
      "Epoch: 34 \tTraining Loss: 0.246641\n",
      "Epoch: 35 \tTraining Loss: 0.269001\n",
      "Epoch: 36 \tTraining Loss: 0.252083\n",
      "Epoch: 37 \tTraining Loss: 0.247358\n",
      "Epoch: 38 \tTraining Loss: 0.268214\n",
      "Epoch: 39 \tTraining Loss: 0.259334\n",
      "Epoch: 40 \tTraining Loss: 0.233758\n",
      "Changing learning rate to 0.025\n",
      "Epoch: 41 \tTraining Loss: 0.252843\n",
      "Epoch: 42 \tTraining Loss: 0.218681\n",
      "Epoch: 43 \tTraining Loss: 0.212626\n",
      "Epoch: 44 \tTraining Loss: 0.209113\n",
      "Epoch: 45 \tTraining Loss: 0.205867\n",
      "Epoch: 46 \tTraining Loss: 0.200946\n",
      "Epoch: 47 \tTraining Loss: 0.197679\n",
      "Epoch: 48 \tTraining Loss: 0.196573\n",
      "Epoch: 49 \tTraining Loss: 0.199749\n",
      "Epoch: 50 \tTraining Loss: 0.199682\n",
      "Epoch: 51 \tTraining Loss: 0.194073\n",
      "Epoch: 52 \tTraining Loss: 0.213873\n",
      "Epoch: 53 \tTraining Loss: 0.202348\n",
      "Epoch: 54 \tTraining Loss: 0.191577\n",
      "Epoch: 55 \tTraining Loss: 0.224406\n",
      "Epoch: 56 \tTraining Loss: 0.209408\n",
      "Epoch: 57 \tTraining Loss: 0.205356\n",
      "Epoch: 58 \tTraining Loss: 0.202028\n",
      "Epoch: 59 \tTraining Loss: 0.206312\n",
      "Epoch: 60 \tTraining Loss: 0.209740\n",
      "Changing learning rate to 0.0125\n",
      "Epoch: 61 \tTraining Loss: 0.235397\n",
      "Epoch: 62 \tTraining Loss: 0.206534\n",
      "Epoch: 63 \tTraining Loss: 0.202258\n",
      "Epoch: 64 \tTraining Loss: 0.203624\n",
      "Epoch: 65 \tTraining Loss: 0.202742\n",
      "Epoch: 66 \tTraining Loss: 0.202927\n",
      "Epoch: 67 \tTraining Loss: 0.205372\n",
      "Epoch: 68 \tTraining Loss: 0.205598\n",
      "Epoch: 69 \tTraining Loss: 0.201505\n",
      "Epoch: 70 \tTraining Loss: 0.195763\n",
      "Epoch: 71 \tTraining Loss: 0.192473\n",
      "Epoch: 72 \tTraining Loss: 0.193494\n",
      "Epoch: 73 \tTraining Loss: 0.196032\n",
      "Epoch: 74 \tTraining Loss: 0.197983\n",
      "Epoch: 75 \tTraining Loss: 0.199578\n",
      "Epoch: 76 \tTraining Loss: 0.201211\n",
      "Epoch: 77 \tTraining Loss: 0.201381\n",
      "Epoch: 78 \tTraining Loss: 0.200876\n",
      "Epoch: 79 \tTraining Loss: 0.200903\n",
      "Epoch: 80 \tTraining Loss: 0.200895\n",
      "Changing learning rate to 0.00625\n",
      "Epoch: 81 \tTraining Loss: 0.200765\n",
      "Epoch: 82 \tTraining Loss: 0.193322\n",
      "Epoch: 83 \tTraining Loss: 0.202592\n",
      "Epoch: 84 \tTraining Loss: 0.201845\n",
      "Epoch: 85 \tTraining Loss: 0.200173\n",
      "Epoch: 86 \tTraining Loss: 0.196079\n",
      "Epoch: 87 \tTraining Loss: 0.191747\n",
      "Epoch: 88 \tTraining Loss: 0.188580\n",
      "Epoch: 89 \tTraining Loss: 0.185172\n",
      "Epoch: 90 \tTraining Loss: 0.181087\n",
      "Epoch: 91 \tTraining Loss: 0.177414\n",
      "Epoch: 92 \tTraining Loss: 0.175980\n",
      "Epoch: 93 \tTraining Loss: 0.175423\n",
      "Epoch: 94 \tTraining Loss: 0.174988\n",
      "Epoch: 95 \tTraining Loss: 0.174673\n",
      "Epoch: 96 \tTraining Loss: 0.174398\n",
      "Epoch: 97 \tTraining Loss: 0.174111\n",
      "Epoch: 98 \tTraining Loss: 0.173800\n",
      "Epoch: 99 \tTraining Loss: 0.173468\n",
      "Epoch: 100 \tTraining Loss: 0.173121\n",
      "Finished training\n"
     ]
    }
   ],
   "source": [
    "h_dim = 4 # Number of neurons in hidden layer\n",
    "\n",
    "model = SRN(1, h_dim) # The configuration of SRN is in:hn-cu:out (1:44:1)\n",
    "\n",
    "print(model)\n",
    "\n",
    "w = w.float().requires_grad_(True)\n",
    "l = l.float().requires_grad_(True)\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "optim = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
    "\n",
    "bce = torch.nn.BCELoss()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    rl = 0.0\n",
    "    i = 0\n",
    "    for f, t in samples:\n",
    "        optim.zero_grad()\n",
    "        \n",
    "        p = model(f)\n",
    "        \n",
    "        loss = bce(p, t)\n",
    "        \n",
    "        loss.backward()\n",
    "\n",
    "        optim.step()\n",
    "        \n",
    "        rl += loss.item()\n",
    "    \n",
    "    # Handcraft learnnig rate decay\n",
    "    if epoch % 20 == 0 and epoch > 0:\n",
    "        for param_group in optim.param_groups:\n",
    "            print(f\"Changing learning rate to {param_group['lr']/2}\")\n",
    "            param_group['lr'] = param_group['lr']/2\n",
    "    \n",
    "    rl /= n_samples\n",
    "    \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f}'.format(epoch+1, rl))\n",
    "        \n",
    "print(\"Finished training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference/Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc:0.92\n"
     ]
    }
   ],
   "source": [
    "# Input test sequence\n",
    "wtest = torch.randint(0, 2, (100, 1,))\n",
    "\n",
    "# output test sequence\n",
    "ltest = xor(wtest)\n",
    "\n",
    "test_samples = list(zip(wtest.float(), ltest.float()))\n",
    "c = 0\n",
    "for f, t in test_samples:\n",
    "    w = round(f.item())\n",
    "    y = round(model(f).item())\n",
    "    t = round(t.item())\n",
    "    \n",
    "    #print(f\"w: {w}, y:{y}, t:{t}\")\n",
    "    \n",
    "    if y == t:\n",
    "        c += 1\n",
    "\n",
    "print(\"acc:\", end= '')        \n",
    "print(c/len(wtest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like some learning has happened.... but I stopped here because the purpose of this task was just to understand recurrence in a simple example using an SRN. \n",
    " \n",
    "More complex models can be implemented using recurrent layers such as [nn.RNN, nn.LSTM, nn.GRU](https://pytorch.org/docs/stable/nn.html) using pytorch or other frameworks.\n",
    "\n",
    "The next steps will focus on understanding the fundamentals of transformer networks with simple examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference:\n",
    "1. https://www.youtube.com/watch?v=ySEx_Bqxvvo&list=PLtBw6njQRU-rwp5__7C0oIVt26ZgjG9NI&index=2\n",
    "2. https://github.com/ramonlins/obsidian/tree/feature/dl/rnn/Papers/recurrent%20networks/Tomas%20Mikolov\n",
    "3. https://github.com/ramonlins/obsidian/blob/feature/dl/rnn/Papers/recurrent%20networks/Tomas%20Mikolov/references/elman1990.pdf\n",
    "4. https://github.com/karpathy/nn-zero-to-hero\n",
    "\n",
    "Additional Materials:\n",
    "\n",
    "https://pabloinsente.github.io/the-recurrent-net"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
