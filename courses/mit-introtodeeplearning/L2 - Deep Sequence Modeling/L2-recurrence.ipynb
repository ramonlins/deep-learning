{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple concept idea\n",
    "Recurrent neural networks models are linked to time and sequencial problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhaps the most simple recurrent network is the Elman Neural Network [2] illustrated in Figure 1.\n",
    "\n",
    "![Elman Net](./images/srn2.png \"Figure 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Elman Net can be denoted as Simple Recurrent Network (SRN) and can be described mathematically [2] as:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "x(t) & =w(t)+s(t-1) \\\\\n",
    "s_j(t) & =f\\left(\\sum_i x_i(t) u_{j i}\\right) \\\\\n",
    "y_k(t) & =g\\left(\\sum_j s_j(t) v_{k j}\\right)\n",
    "\\end{aligned}\n",
    "$$\n",
    "The sigmoid activation function:\n",
    "$$\n",
    "f(z)=\\frac{1}{1+e^{-z}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where:\n",
    "\n",
    "- $x(t)$ is the input at time $t$ formed by concatenating:\n",
    "    - $w(t)$ represents the current word;\n",
    "    - $s(t-1)$ is the output from neurons in context layer $s$ at time $t-1$;\n",
    "- $s_j(t)$ is the context output at time $t$ with $j$ (#hidden neurons);\n",
    "- $y_k(t)$ is the output at time $t$ with $k$ (output dim);\n",
    "- $u_{ij}$ represents the input weights with $i$ (input dim);\n",
    "- $v_{kj}$ represents the output weights bias;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think this representation is very similar to the representation shown in [mit-lecture2-RNNs](https://www.youtube.com/watch?v=ySEx_Bqxvvo&list=PLtBw6njQRU-rwp5__7C0oIVt26ZgjG9NI&index=3) with recurrent cell and unfold illustration through time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In an attempt to understand basic fundamentals of recurrent nets, I created a network from scratch to try to solve the xor problem as a sequence as proposed by Elman [3]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The sequence XOR Problem\n",
    "Input : 1010001110101 ...\n",
    "\n",
    "Output: 011100100111? ...\n",
    "\n",
    "When the network has received the first bit-1 in the example above-there is a 50% chance that the next bit will be a 1 (or a 0). When the network receives the second bit (0),\n",
    "however, it should then be possible to predict that the third will be the XOR, 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Reproducibility\n",
    "torch.manual_seed(13)\n",
    "\n",
    "# Create output XOR sequence\n",
    "def xor(w):\n",
    "    # The first element of sequence has 50% of chance of happens\n",
    "    y = torch.randint(0, 2, (1,))\n",
    "    \n",
    "    # The next elements consider two time steps\n",
    "    for i in range(len(w)-1):\n",
    "        y = torch.cat((y, w[i] ^ w[i+1]), 0) # xor sequence\n",
    "\n",
    "    # Add batch dimension \n",
    "    y = y.view(y.shape[0], 1)\n",
    "    \n",
    "    return y\n",
    "\n",
    "# Define random sequence input\n",
    "n_samples = 300\n",
    "w = torch.randint(0, 2, (n_samples, 1,))\n",
    "\n",
    "# Get target output\n",
    "l = xor(w)\n",
    "\n",
    "# Concatenate the data\n",
    "samples = list(zip(w.float(), l.float()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For debug, uncomment to track wich node autograd has problem \n",
    "#torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "class SRN(torch.nn.Module):\n",
    "    def __init__(self, w_dim, h_dim):\n",
    "        super().__init__()\n",
    "        self.hx = torch.nn.Linear(w_dim + h_dim, h_dim)  # Hidden layer with input (w) + context units (s-1)        \n",
    "        self.ho = torch.nn.Linear(h_dim, 1)\n",
    "        self.s_ = torch.rand(h_dim) # Random initialize context units, hence: \n",
    "                                    # for t=0 and h_dim=2 -> s(t-1) = [s0(-1), s1(-1)]\n",
    "        # Activation function\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, w): \n",
    "        e = torch.cat((w, self.s_)) # w(t) + s(t-1)\n",
    "        s = self.sigmoid(self.hx(e)) # s(t)\n",
    "        o = self.ho(s)               # logits\n",
    "        y = self.sigmoid(o)         # probs\n",
    "        \n",
    "        self.s_ = s.detach()        # old context\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SRN(\n",
      "  (hx): Linear(in_features=9, out_features=8, bias=True)\n",
      "  (ho): Linear(in_features=8, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n",
      "Epoch: 1 \tTraining Loss: 0.773967\n",
      "Epoch: 2 \tTraining Loss: 0.742038\n",
      "Epoch: 3 \tTraining Loss: 0.732925\n",
      "Epoch: 4 \tTraining Loss: 0.718082\n",
      "Epoch: 5 \tTraining Loss: 0.688876\n",
      "Epoch: 6 \tTraining Loss: 0.655052\n",
      "Epoch: 7 \tTraining Loss: 0.574897\n",
      "Epoch: 8 \tTraining Loss: 0.500076\n",
      "Epoch: 9 \tTraining Loss: 0.141949\n",
      "Epoch: 10 \tTraining Loss: 0.086450\n",
      "Epoch: 11 \tTraining Loss: 0.203110\n",
      "Epoch: 12 \tTraining Loss: 0.037135\n",
      "Epoch: 13 \tTraining Loss: 0.034361\n",
      "Epoch: 14 \tTraining Loss: 0.030730\n",
      "Epoch: 15 \tTraining Loss: 0.027986\n",
      "Epoch: 16 \tTraining Loss: 0.026380\n",
      "Epoch: 17 \tTraining Loss: 0.025420\n",
      "Epoch: 18 \tTraining Loss: 0.024767\n",
      "Epoch: 19 \tTraining Loss: 0.024265\n",
      "Epoch: 20 \tTraining Loss: 0.023863\n",
      "Epoch: 21 \tTraining Loss: 0.023534\n",
      "Epoch: 22 \tTraining Loss: 0.023260\n",
      "Epoch: 23 \tTraining Loss: 0.023026\n",
      "Epoch: 24 \tTraining Loss: 0.022820\n",
      "Epoch: 25 \tTraining Loss: 0.022633\n",
      "Epoch: 26 \tTraining Loss: 0.022458\n",
      "Epoch: 27 \tTraining Loss: 0.022288\n",
      "Epoch: 28 \tTraining Loss: 0.022117\n",
      "Epoch: 29 \tTraining Loss: 0.021941\n",
      "Epoch: 30 \tTraining Loss: 0.021760\n",
      "Epoch: 31 \tTraining Loss: 0.021578\n",
      "Epoch: 32 \tTraining Loss: 0.021405\n",
      "Epoch: 33 \tTraining Loss: 0.021248\n",
      "Epoch: 34 \tTraining Loss: 0.021102\n",
      "Epoch: 35 \tTraining Loss: 0.020953\n",
      "Epoch: 36 \tTraining Loss: 0.020794\n",
      "Epoch: 37 \tTraining Loss: 0.020619\n",
      "Epoch: 38 \tTraining Loss: 0.020419\n",
      "Epoch: 39 \tTraining Loss: 0.020190\n",
      "Epoch: 40 \tTraining Loss: 0.019967\n",
      "Epoch: 41 \tTraining Loss: 0.019847\n",
      "Epoch: 42 \tTraining Loss: 0.019769\n",
      "Epoch: 43 \tTraining Loss: 0.019655\n",
      "Epoch: 44 \tTraining Loss: 0.019577\n",
      "Epoch: 45 \tTraining Loss: 0.019481\n",
      "Epoch: 46 \tTraining Loss: 0.019412\n",
      "Epoch: 47 \tTraining Loss: 0.019324\n",
      "Epoch: 48 \tTraining Loss: 0.019261\n",
      "Epoch: 49 \tTraining Loss: 0.019177\n",
      "Epoch: 50 \tTraining Loss: 0.019116\n",
      "Epoch: 51 \tTraining Loss: 0.019033\n",
      "Epoch: 52 \tTraining Loss: 0.018970\n",
      "Epoch: 53 \tTraining Loss: 0.018887\n",
      "Epoch: 54 \tTraining Loss: 0.018826\n",
      "Epoch: 55 \tTraining Loss: 0.018752\n",
      "Epoch: 56 \tTraining Loss: 0.018705\n",
      "Epoch: 57 \tTraining Loss: 0.018651\n",
      "Epoch: 58 \tTraining Loss: 0.018618\n",
      "Epoch: 59 \tTraining Loss: 0.018576\n",
      "Epoch: 60 \tTraining Loss: 0.018550\n",
      "Epoch: 61 \tTraining Loss: 0.018514\n",
      "Epoch: 62 \tTraining Loss: 0.018492\n",
      "Epoch: 63 \tTraining Loss: 0.018462\n",
      "Epoch: 64 \tTraining Loss: 0.018443\n",
      "Epoch: 65 \tTraining Loss: 0.018417\n",
      "Epoch: 66 \tTraining Loss: 0.018400\n",
      "Epoch: 67 \tTraining Loss: 0.018378\n",
      "Epoch: 68 \tTraining Loss: 0.018363\n",
      "Epoch: 69 \tTraining Loss: 0.018344\n",
      "Epoch: 70 \tTraining Loss: 0.018330\n",
      "Epoch: 71 \tTraining Loss: 0.018313\n",
      "Epoch: 72 \tTraining Loss: 0.018301\n",
      "Epoch: 73 \tTraining Loss: 0.018286\n",
      "Epoch: 74 \tTraining Loss: 0.018275\n",
      "Epoch: 75 \tTraining Loss: 0.018261\n",
      "Epoch: 76 \tTraining Loss: 0.018251\n",
      "Epoch: 77 \tTraining Loss: 0.018239\n",
      "Epoch: 78 \tTraining Loss: 0.018229\n",
      "Epoch: 79 \tTraining Loss: 0.018219\n",
      "Epoch: 80 \tTraining Loss: 0.018210\n",
      "Epoch: 81 \tTraining Loss: 0.018200\n",
      "Epoch: 82 \tTraining Loss: 0.018192\n",
      "Epoch: 83 \tTraining Loss: 0.018183\n",
      "Epoch: 84 \tTraining Loss: 0.018176\n",
      "Epoch: 85 \tTraining Loss: 0.018168\n",
      "Epoch: 86 \tTraining Loss: 0.018161\n",
      "Epoch: 87 \tTraining Loss: 0.018153\n",
      "Epoch: 88 \tTraining Loss: 0.018147\n",
      "Epoch: 89 \tTraining Loss: 0.018140\n",
      "Epoch: 90 \tTraining Loss: 0.018134\n",
      "Epoch: 91 \tTraining Loss: 0.018128\n",
      "Epoch: 92 \tTraining Loss: 0.018122\n",
      "Epoch: 93 \tTraining Loss: 0.018116\n",
      "Epoch: 94 \tTraining Loss: 0.018111\n",
      "Epoch: 95 \tTraining Loss: 0.018106\n",
      "Epoch: 96 \tTraining Loss: 0.018101\n",
      "Epoch: 97 \tTraining Loss: 0.018096\n",
      "Epoch: 98 \tTraining Loss: 0.018091\n",
      "Epoch: 99 \tTraining Loss: 0.018087\n",
      "Epoch: 100 \tTraining Loss: 0.018082\n",
      "Epoch: 101 \tTraining Loss: 0.018078\n",
      "Epoch: 102 \tTraining Loss: 0.018074\n",
      "Epoch: 103 \tTraining Loss: 0.018070\n",
      "Epoch: 104 \tTraining Loss: 0.018066\n",
      "Epoch: 105 \tTraining Loss: 0.018062\n",
      "Epoch: 106 \tTraining Loss: 0.018059\n",
      "Epoch: 107 \tTraining Loss: 0.018055\n",
      "Epoch: 108 \tTraining Loss: 0.018052\n",
      "Epoch: 109 \tTraining Loss: 0.018048\n",
      "Epoch: 110 \tTraining Loss: 0.018045\n",
      "Epoch: 111 \tTraining Loss: 0.018042\n",
      "Epoch: 112 \tTraining Loss: 0.018039\n",
      "Epoch: 113 \tTraining Loss: 0.018036\n",
      "Epoch: 114 \tTraining Loss: 0.018034\n",
      "Epoch: 115 \tTraining Loss: 0.018031\n",
      "Epoch: 116 \tTraining Loss: 0.018028\n",
      "Epoch: 117 \tTraining Loss: 0.018026\n",
      "Epoch: 118 \tTraining Loss: 0.018023\n",
      "Epoch: 119 \tTraining Loss: 0.018021\n",
      "Epoch: 120 \tTraining Loss: 0.018018\n",
      "Changing learning rate to 0.05\n",
      "Epoch: 121 \tTraining Loss: 0.018016\n",
      "Epoch: 122 \tTraining Loss: 0.017176\n",
      "Epoch: 123 \tTraining Loss: 0.016032\n",
      "Epoch: 124 \tTraining Loss: 0.015739\n",
      "Epoch: 125 \tTraining Loss: 0.015751\n",
      "Epoch: 126 \tTraining Loss: 0.015779\n",
      "Epoch: 127 \tTraining Loss: 0.015795\n",
      "Epoch: 128 \tTraining Loss: 0.015804\n",
      "Epoch: 129 \tTraining Loss: 0.015809\n",
      "Epoch: 130 \tTraining Loss: 0.015812\n",
      "Epoch: 131 \tTraining Loss: 0.015813\n",
      "Epoch: 132 \tTraining Loss: 0.015814\n",
      "Epoch: 133 \tTraining Loss: 0.015814\n",
      "Epoch: 134 \tTraining Loss: 0.015814\n",
      "Epoch: 135 \tTraining Loss: 0.015814\n",
      "Epoch: 136 \tTraining Loss: 0.015813\n",
      "Epoch: 137 \tTraining Loss: 0.015812\n",
      "Epoch: 138 \tTraining Loss: 0.015811\n",
      "Epoch: 139 \tTraining Loss: 0.015810\n",
      "Epoch: 140 \tTraining Loss: 0.015809\n",
      "Epoch: 141 \tTraining Loss: 0.015808\n",
      "Epoch: 142 \tTraining Loss: 0.015806\n",
      "Epoch: 143 \tTraining Loss: 0.015805\n",
      "Epoch: 144 \tTraining Loss: 0.015804\n",
      "Epoch: 145 \tTraining Loss: 0.015802\n",
      "Epoch: 146 \tTraining Loss: 0.015801\n",
      "Epoch: 147 \tTraining Loss: 0.015799\n",
      "Epoch: 148 \tTraining Loss: 0.015798\n",
      "Epoch: 149 \tTraining Loss: 0.015797\n",
      "Epoch: 150 \tTraining Loss: 0.015795\n",
      "Epoch: 151 \tTraining Loss: 0.015794\n",
      "Epoch: 152 \tTraining Loss: 0.015793\n",
      "Epoch: 153 \tTraining Loss: 0.015791\n",
      "Epoch: 154 \tTraining Loss: 0.015790\n",
      "Epoch: 155 \tTraining Loss: 0.015789\n",
      "Epoch: 156 \tTraining Loss: 0.015788\n",
      "Epoch: 157 \tTraining Loss: 0.015786\n",
      "Epoch: 158 \tTraining Loss: 0.015785\n",
      "Epoch: 159 \tTraining Loss: 0.015784\n",
      "Epoch: 160 \tTraining Loss: 0.015783\n",
      "Epoch: 161 \tTraining Loss: 0.015782\n",
      "Epoch: 162 \tTraining Loss: 0.015781\n",
      "Epoch: 163 \tTraining Loss: 0.015780\n",
      "Epoch: 164 \tTraining Loss: 0.015779\n",
      "Epoch: 165 \tTraining Loss: 0.015778\n",
      "Epoch: 166 \tTraining Loss: 0.015777\n",
      "Epoch: 167 \tTraining Loss: 0.015776\n",
      "Epoch: 168 \tTraining Loss: 0.015775\n",
      "Epoch: 169 \tTraining Loss: 0.015774\n",
      "Epoch: 170 \tTraining Loss: 0.015773\n",
      "Epoch: 171 \tTraining Loss: 0.015772\n",
      "Epoch: 172 \tTraining Loss: 0.015771\n",
      "Epoch: 173 \tTraining Loss: 0.015770\n",
      "Epoch: 174 \tTraining Loss: 0.015769\n",
      "Epoch: 175 \tTraining Loss: 0.015769\n",
      "Epoch: 176 \tTraining Loss: 0.015768\n",
      "Epoch: 177 \tTraining Loss: 0.015767\n",
      "Epoch: 178 \tTraining Loss: 0.015766\n",
      "Epoch: 179 \tTraining Loss: 0.015766\n",
      "Epoch: 180 \tTraining Loss: 0.015765\n",
      "Epoch: 181 \tTraining Loss: 0.015764\n",
      "Epoch: 182 \tTraining Loss: 0.015764\n",
      "Epoch: 183 \tTraining Loss: 0.015763\n",
      "Epoch: 184 \tTraining Loss: 0.015762\n",
      "Epoch: 185 \tTraining Loss: 0.015762\n",
      "Epoch: 186 \tTraining Loss: 0.015761\n",
      "Epoch: 187 \tTraining Loss: 0.015760\n",
      "Epoch: 188 \tTraining Loss: 0.015760\n",
      "Epoch: 189 \tTraining Loss: 0.015759\n",
      "Epoch: 190 \tTraining Loss: 0.015759\n",
      "Epoch: 191 \tTraining Loss: 0.015758\n",
      "Epoch: 192 \tTraining Loss: 0.015758\n",
      "Epoch: 193 \tTraining Loss: 0.015757\n",
      "Epoch: 194 \tTraining Loss: 0.015757\n",
      "Epoch: 195 \tTraining Loss: 0.015756\n",
      "Epoch: 196 \tTraining Loss: 0.015755\n",
      "Epoch: 197 \tTraining Loss: 0.015755\n",
      "Epoch: 198 \tTraining Loss: 0.015754\n",
      "Epoch: 199 \tTraining Loss: 0.015754\n",
      "Epoch: 200 \tTraining Loss: 0.015753\n",
      "Epoch: 201 \tTraining Loss: 0.015753\n",
      "Epoch: 202 \tTraining Loss: 0.015752\n",
      "Epoch: 203 \tTraining Loss: 0.015752\n",
      "Epoch: 204 \tTraining Loss: 0.015751\n",
      "Epoch: 205 \tTraining Loss: 0.015751\n",
      "Epoch: 206 \tTraining Loss: 0.015750\n",
      "Epoch: 207 \tTraining Loss: 0.015750\n",
      "Epoch: 208 \tTraining Loss: 0.015749\n",
      "Epoch: 209 \tTraining Loss: 0.015748\n",
      "Epoch: 210 \tTraining Loss: 0.015748\n",
      "Epoch: 211 \tTraining Loss: 0.015747\n",
      "Epoch: 212 \tTraining Loss: 0.015747\n",
      "Epoch: 213 \tTraining Loss: 0.015746\n",
      "Epoch: 214 \tTraining Loss: 0.015746\n",
      "Epoch: 215 \tTraining Loss: 0.015745\n",
      "Epoch: 216 \tTraining Loss: 0.015744\n",
      "Epoch: 217 \tTraining Loss: 0.015744\n",
      "Epoch: 218 \tTraining Loss: 0.015743\n",
      "Epoch: 219 \tTraining Loss: 0.015742\n",
      "Epoch: 220 \tTraining Loss: 0.015742\n",
      "Epoch: 221 \tTraining Loss: 0.015741\n",
      "Epoch: 222 \tTraining Loss: 0.015740\n",
      "Epoch: 223 \tTraining Loss: 0.015740\n",
      "Epoch: 224 \tTraining Loss: 0.015739\n",
      "Epoch: 225 \tTraining Loss: 0.015738\n",
      "Epoch: 226 \tTraining Loss: 0.015737\n",
      "Epoch: 227 \tTraining Loss: 0.015737\n",
      "Epoch: 228 \tTraining Loss: 0.015736\n",
      "Epoch: 229 \tTraining Loss: 0.015735\n",
      "Epoch: 230 \tTraining Loss: 0.015734\n",
      "Epoch: 231 \tTraining Loss: 0.015733\n",
      "Epoch: 232 \tTraining Loss: 0.015732\n",
      "Epoch: 233 \tTraining Loss: 0.015731\n",
      "Epoch: 234 \tTraining Loss: 0.015731\n",
      "Epoch: 235 \tTraining Loss: 0.015730\n",
      "Epoch: 236 \tTraining Loss: 0.015729\n",
      "Epoch: 237 \tTraining Loss: 0.015728\n",
      "Epoch: 238 \tTraining Loss: 0.015727\n",
      "Epoch: 239 \tTraining Loss: 0.015726\n",
      "Epoch: 240 \tTraining Loss: 0.015725\n",
      "Changing learning rate to 0.025\n",
      "Epoch: 241 \tTraining Loss: 0.015723\n",
      "Epoch: 242 \tTraining Loss: 0.014952\n",
      "Epoch: 243 \tTraining Loss: 0.014278\n",
      "Epoch: 244 \tTraining Loss: 0.014090\n",
      "Epoch: 245 \tTraining Loss: 0.014088\n",
      "Epoch: 246 \tTraining Loss: 0.014114\n",
      "Epoch: 247 \tTraining Loss: 0.014139\n",
      "Epoch: 248 \tTraining Loss: 0.014160\n",
      "Epoch: 249 \tTraining Loss: 0.014177\n",
      "Epoch: 250 \tTraining Loss: 0.014190\n",
      "Epoch: 251 \tTraining Loss: 0.014199\n",
      "Epoch: 252 \tTraining Loss: 0.014207\n",
      "Epoch: 253 \tTraining Loss: 0.014212\n",
      "Epoch: 254 \tTraining Loss: 0.014216\n",
      "Epoch: 255 \tTraining Loss: 0.014220\n",
      "Epoch: 256 \tTraining Loss: 0.014222\n",
      "Epoch: 257 \tTraining Loss: 0.014224\n",
      "Epoch: 258 \tTraining Loss: 0.014226\n",
      "Epoch: 259 \tTraining Loss: 0.014227\n",
      "Epoch: 260 \tTraining Loss: 0.014229\n",
      "Epoch: 261 \tTraining Loss: 0.014230\n",
      "Epoch: 262 \tTraining Loss: 0.014231\n",
      "Epoch: 263 \tTraining Loss: 0.014232\n",
      "Epoch: 264 \tTraining Loss: 0.014232\n",
      "Epoch: 265 \tTraining Loss: 0.014233\n",
      "Epoch: 266 \tTraining Loss: 0.014234\n",
      "Epoch: 267 \tTraining Loss: 0.014235\n",
      "Epoch: 268 \tTraining Loss: 0.014235\n",
      "Epoch: 269 \tTraining Loss: 0.014236\n",
      "Epoch: 270 \tTraining Loss: 0.014236\n",
      "Epoch: 271 \tTraining Loss: 0.014237\n",
      "Epoch: 272 \tTraining Loss: 0.014237\n",
      "Epoch: 273 \tTraining Loss: 0.014237\n",
      "Epoch: 274 \tTraining Loss: 0.014238\n",
      "Epoch: 275 \tTraining Loss: 0.014238\n",
      "Epoch: 276 \tTraining Loss: 0.014238\n",
      "Epoch: 277 \tTraining Loss: 0.014239\n",
      "Epoch: 278 \tTraining Loss: 0.014239\n",
      "Epoch: 279 \tTraining Loss: 0.014239\n",
      "Epoch: 280 \tTraining Loss: 0.014239\n",
      "Epoch: 281 \tTraining Loss: 0.014240\n",
      "Epoch: 282 \tTraining Loss: 0.014240\n",
      "Epoch: 283 \tTraining Loss: 0.014240\n",
      "Epoch: 284 \tTraining Loss: 0.014240\n",
      "Epoch: 285 \tTraining Loss: 0.014240\n",
      "Epoch: 286 \tTraining Loss: 0.014240\n",
      "Epoch: 287 \tTraining Loss: 0.014240\n",
      "Epoch: 288 \tTraining Loss: 0.014240\n",
      "Epoch: 289 \tTraining Loss: 0.014240\n",
      "Epoch: 290 \tTraining Loss: 0.014240\n",
      "Epoch: 291 \tTraining Loss: 0.014240\n",
      "Epoch: 292 \tTraining Loss: 0.014240\n",
      "Epoch: 293 \tTraining Loss: 0.014240\n",
      "Epoch: 294 \tTraining Loss: 0.014240\n",
      "Epoch: 295 \tTraining Loss: 0.014240\n",
      "Epoch: 296 \tTraining Loss: 0.014239\n",
      "Epoch: 297 \tTraining Loss: 0.014239\n",
      "Epoch: 298 \tTraining Loss: 0.014239\n",
      "Epoch: 299 \tTraining Loss: 0.014239\n",
      "Epoch: 300 \tTraining Loss: 0.014239\n",
      "Epoch: 301 \tTraining Loss: 0.014239\n",
      "Epoch: 302 \tTraining Loss: 0.014238\n",
      "Epoch: 303 \tTraining Loss: 0.014238\n",
      "Epoch: 304 \tTraining Loss: 0.014238\n",
      "Epoch: 305 \tTraining Loss: 0.014238\n",
      "Epoch: 306 \tTraining Loss: 0.014237\n",
      "Epoch: 307 \tTraining Loss: 0.014237\n",
      "Epoch: 308 \tTraining Loss: 0.014237\n",
      "Epoch: 309 \tTraining Loss: 0.014236\n",
      "Epoch: 310 \tTraining Loss: 0.014236\n",
      "Epoch: 311 \tTraining Loss: 0.014236\n",
      "Epoch: 312 \tTraining Loss: 0.014235\n",
      "Epoch: 313 \tTraining Loss: 0.014235\n",
      "Epoch: 314 \tTraining Loss: 0.014234\n",
      "Epoch: 315 \tTraining Loss: 0.014234\n",
      "Epoch: 316 \tTraining Loss: 0.014234\n",
      "Epoch: 317 \tTraining Loss: 0.014233\n",
      "Epoch: 318 \tTraining Loss: 0.014233\n",
      "Epoch: 319 \tTraining Loss: 0.014232\n",
      "Epoch: 320 \tTraining Loss: 0.014232\n",
      "Epoch: 321 \tTraining Loss: 0.014231\n",
      "Epoch: 322 \tTraining Loss: 0.014231\n",
      "Epoch: 323 \tTraining Loss: 0.014230\n",
      "Epoch: 324 \tTraining Loss: 0.014230\n",
      "Epoch: 325 \tTraining Loss: 0.014229\n",
      "Epoch: 326 \tTraining Loss: 0.014229\n",
      "Epoch: 327 \tTraining Loss: 0.014228\n",
      "Epoch: 328 \tTraining Loss: 0.014228\n",
      "Epoch: 329 \tTraining Loss: 0.014227\n",
      "Epoch: 330 \tTraining Loss: 0.014227\n",
      "Epoch: 331 \tTraining Loss: 0.014226\n",
      "Epoch: 332 \tTraining Loss: 0.014225\n",
      "Epoch: 333 \tTraining Loss: 0.014225\n",
      "Epoch: 334 \tTraining Loss: 0.014224\n",
      "Epoch: 335 \tTraining Loss: 0.014224\n",
      "Epoch: 336 \tTraining Loss: 0.014223\n",
      "Epoch: 337 \tTraining Loss: 0.014222\n",
      "Epoch: 338 \tTraining Loss: 0.014222\n",
      "Epoch: 339 \tTraining Loss: 0.014221\n",
      "Epoch: 340 \tTraining Loss: 0.014220\n",
      "Epoch: 341 \tTraining Loss: 0.014220\n",
      "Epoch: 342 \tTraining Loss: 0.014219\n",
      "Epoch: 343 \tTraining Loss: 0.014218\n",
      "Epoch: 344 \tTraining Loss: 0.014218\n",
      "Epoch: 345 \tTraining Loss: 0.014217\n",
      "Epoch: 346 \tTraining Loss: 0.014216\n",
      "Epoch: 347 \tTraining Loss: 0.014215\n",
      "Epoch: 348 \tTraining Loss: 0.014215\n",
      "Epoch: 349 \tTraining Loss: 0.014214\n",
      "Epoch: 350 \tTraining Loss: 0.014213\n",
      "Epoch: 351 \tTraining Loss: 0.014212\n",
      "Epoch: 352 \tTraining Loss: 0.014212\n",
      "Epoch: 353 \tTraining Loss: 0.014211\n",
      "Epoch: 354 \tTraining Loss: 0.014210\n",
      "Epoch: 355 \tTraining Loss: 0.014209\n",
      "Epoch: 356 \tTraining Loss: 0.014208\n",
      "Epoch: 357 \tTraining Loss: 0.014208\n",
      "Epoch: 358 \tTraining Loss: 0.014207\n",
      "Epoch: 359 \tTraining Loss: 0.014206\n",
      "Epoch: 360 \tTraining Loss: 0.014205\n",
      "Changing learning rate to 0.0125\n",
      "Epoch: 361 \tTraining Loss: 0.014204\n",
      "Epoch: 362 \tTraining Loss: 0.013491\n",
      "Epoch: 363 \tTraining Loss: 0.013115\n",
      "Epoch: 364 \tTraining Loss: 0.012972\n",
      "Epoch: 365 \tTraining Loss: 0.012938\n",
      "Epoch: 366 \tTraining Loss: 0.012940\n",
      "Epoch: 367 \tTraining Loss: 0.012953\n",
      "Epoch: 368 \tTraining Loss: 0.012967\n",
      "Epoch: 369 \tTraining Loss: 0.012982\n",
      "Epoch: 370 \tTraining Loss: 0.012995\n",
      "Epoch: 371 \tTraining Loss: 0.013008\n",
      "Epoch: 372 \tTraining Loss: 0.013020\n",
      "Epoch: 373 \tTraining Loss: 0.013030\n",
      "Epoch: 374 \tTraining Loss: 0.013040\n",
      "Epoch: 375 \tTraining Loss: 0.013048\n",
      "Epoch: 376 \tTraining Loss: 0.013055\n",
      "Epoch: 377 \tTraining Loss: 0.013062\n",
      "Epoch: 378 \tTraining Loss: 0.013067\n",
      "Epoch: 379 \tTraining Loss: 0.013072\n",
      "Epoch: 380 \tTraining Loss: 0.013077\n",
      "Epoch: 381 \tTraining Loss: 0.013080\n",
      "Epoch: 382 \tTraining Loss: 0.013084\n",
      "Epoch: 383 \tTraining Loss: 0.013087\n",
      "Epoch: 384 \tTraining Loss: 0.013089\n",
      "Epoch: 385 \tTraining Loss: 0.013092\n",
      "Epoch: 386 \tTraining Loss: 0.013094\n",
      "Epoch: 387 \tTraining Loss: 0.013095\n",
      "Epoch: 388 \tTraining Loss: 0.013097\n",
      "Epoch: 389 \tTraining Loss: 0.013098\n",
      "Epoch: 390 \tTraining Loss: 0.013100\n",
      "Epoch: 391 \tTraining Loss: 0.013101\n",
      "Epoch: 392 \tTraining Loss: 0.013102\n",
      "Epoch: 393 \tTraining Loss: 0.013103\n",
      "Epoch: 394 \tTraining Loss: 0.013103\n",
      "Epoch: 395 \tTraining Loss: 0.013104\n",
      "Epoch: 396 \tTraining Loss: 0.013105\n",
      "Epoch: 397 \tTraining Loss: 0.013105\n",
      "Epoch: 398 \tTraining Loss: 0.013106\n",
      "Epoch: 399 \tTraining Loss: 0.013107\n",
      "Epoch: 400 \tTraining Loss: 0.013107\n",
      "Epoch: 401 \tTraining Loss: 0.013108\n",
      "Epoch: 402 \tTraining Loss: 0.013108\n",
      "Epoch: 403 \tTraining Loss: 0.013109\n",
      "Epoch: 404 \tTraining Loss: 0.013109\n",
      "Epoch: 405 \tTraining Loss: 0.013110\n",
      "Epoch: 406 \tTraining Loss: 0.013110\n",
      "Epoch: 407 \tTraining Loss: 0.013110\n",
      "Epoch: 408 \tTraining Loss: 0.013111\n",
      "Epoch: 409 \tTraining Loss: 0.013111\n",
      "Epoch: 410 \tTraining Loss: 0.013111\n",
      "Epoch: 411 \tTraining Loss: 0.013112\n",
      "Epoch: 412 \tTraining Loss: 0.013112\n",
      "Epoch: 413 \tTraining Loss: 0.013112\n",
      "Epoch: 414 \tTraining Loss: 0.013113\n",
      "Epoch: 415 \tTraining Loss: 0.013113\n",
      "Epoch: 416 \tTraining Loss: 0.013113\n",
      "Epoch: 417 \tTraining Loss: 0.013114\n",
      "Epoch: 418 \tTraining Loss: 0.013114\n",
      "Epoch: 419 \tTraining Loss: 0.013114\n",
      "Epoch: 420 \tTraining Loss: 0.013114\n",
      "Epoch: 421 \tTraining Loss: 0.013115\n",
      "Epoch: 422 \tTraining Loss: 0.013115\n",
      "Epoch: 423 \tTraining Loss: 0.013115\n",
      "Epoch: 424 \tTraining Loss: 0.013115\n",
      "Epoch: 425 \tTraining Loss: 0.013116\n",
      "Epoch: 426 \tTraining Loss: 0.013116\n",
      "Epoch: 427 \tTraining Loss: 0.013116\n",
      "Epoch: 428 \tTraining Loss: 0.013116\n",
      "Epoch: 429 \tTraining Loss: 0.013117\n",
      "Epoch: 430 \tTraining Loss: 0.013117\n",
      "Epoch: 431 \tTraining Loss: 0.013117\n",
      "Epoch: 432 \tTraining Loss: 0.013117\n",
      "Epoch: 433 \tTraining Loss: 0.013118\n",
      "Epoch: 434 \tTraining Loss: 0.013118\n",
      "Epoch: 435 \tTraining Loss: 0.013118\n",
      "Epoch: 436 \tTraining Loss: 0.013118\n",
      "Epoch: 437 \tTraining Loss: 0.013118\n",
      "Epoch: 438 \tTraining Loss: 0.013118\n",
      "Epoch: 439 \tTraining Loss: 0.013119\n",
      "Epoch: 440 \tTraining Loss: 0.013119\n",
      "Epoch: 441 \tTraining Loss: 0.013119\n",
      "Epoch: 442 \tTraining Loss: 0.013119\n",
      "Epoch: 443 \tTraining Loss: 0.013119\n",
      "Epoch: 444 \tTraining Loss: 0.013120\n",
      "Epoch: 445 \tTraining Loss: 0.013120\n",
      "Epoch: 446 \tTraining Loss: 0.013120\n",
      "Epoch: 447 \tTraining Loss: 0.013120\n",
      "Epoch: 448 \tTraining Loss: 0.013120\n",
      "Epoch: 449 \tTraining Loss: 0.013120\n",
      "Epoch: 450 \tTraining Loss: 0.013120\n",
      "Epoch: 451 \tTraining Loss: 0.013121\n",
      "Epoch: 452 \tTraining Loss: 0.013121\n",
      "Epoch: 453 \tTraining Loss: 0.013121\n",
      "Epoch: 454 \tTraining Loss: 0.013121\n",
      "Epoch: 455 \tTraining Loss: 0.013121\n",
      "Epoch: 456 \tTraining Loss: 0.013121\n",
      "Epoch: 457 \tTraining Loss: 0.013121\n",
      "Epoch: 458 \tTraining Loss: 0.013121\n",
      "Epoch: 459 \tTraining Loss: 0.013122\n",
      "Epoch: 460 \tTraining Loss: 0.013122\n",
      "Epoch: 461 \tTraining Loss: 0.013122\n",
      "Epoch: 462 \tTraining Loss: 0.013122\n",
      "Epoch: 463 \tTraining Loss: 0.013122\n",
      "Epoch: 464 \tTraining Loss: 0.013122\n",
      "Epoch: 465 \tTraining Loss: 0.013122\n",
      "Epoch: 466 \tTraining Loss: 0.013122\n",
      "Epoch: 467 \tTraining Loss: 0.013122\n",
      "Epoch: 468 \tTraining Loss: 0.013122\n",
      "Epoch: 469 \tTraining Loss: 0.013122\n",
      "Epoch: 470 \tTraining Loss: 0.013122\n",
      "Epoch: 471 \tTraining Loss: 0.013122\n",
      "Epoch: 472 \tTraining Loss: 0.013123\n",
      "Epoch: 473 \tTraining Loss: 0.013123\n",
      "Epoch: 474 \tTraining Loss: 0.013123\n",
      "Epoch: 475 \tTraining Loss: 0.013123\n",
      "Epoch: 476 \tTraining Loss: 0.013123\n",
      "Epoch: 477 \tTraining Loss: 0.013123\n",
      "Epoch: 478 \tTraining Loss: 0.013123\n",
      "Epoch: 479 \tTraining Loss: 0.013123\n",
      "Epoch: 480 \tTraining Loss: 0.013123\n",
      "Changing learning rate to 0.00625\n",
      "Epoch: 481 \tTraining Loss: 0.013123\n",
      "Epoch: 482 \tTraining Loss: 0.012479\n",
      "Epoch: 483 \tTraining Loss: 0.012256\n",
      "Epoch: 484 \tTraining Loss: 0.012153\n",
      "Epoch: 485 \tTraining Loss: 0.012112\n",
      "Epoch: 486 \tTraining Loss: 0.012099\n",
      "Epoch: 487 \tTraining Loss: 0.012097\n",
      "Epoch: 488 \tTraining Loss: 0.012100\n",
      "Epoch: 489 \tTraining Loss: 0.012104\n",
      "Epoch: 490 \tTraining Loss: 0.012110\n",
      "Epoch: 491 \tTraining Loss: 0.012115\n",
      "Epoch: 492 \tTraining Loss: 0.012121\n",
      "Epoch: 493 \tTraining Loss: 0.012127\n",
      "Epoch: 494 \tTraining Loss: 0.012133\n",
      "Epoch: 495 \tTraining Loss: 0.012139\n",
      "Epoch: 496 \tTraining Loss: 0.012144\n",
      "Epoch: 497 \tTraining Loss: 0.012150\n",
      "Epoch: 498 \tTraining Loss: 0.012156\n",
      "Epoch: 499 \tTraining Loss: 0.012161\n",
      "Epoch: 500 \tTraining Loss: 0.012166\n",
      "Epoch: 501 \tTraining Loss: 0.012171\n",
      "Epoch: 502 \tTraining Loss: 0.012176\n",
      "Epoch: 503 \tTraining Loss: 0.012181\n",
      "Epoch: 504 \tTraining Loss: 0.012186\n",
      "Epoch: 505 \tTraining Loss: 0.012190\n",
      "Epoch: 506 \tTraining Loss: 0.012195\n",
      "Epoch: 507 \tTraining Loss: 0.012199\n",
      "Epoch: 508 \tTraining Loss: 0.012203\n",
      "Epoch: 509 \tTraining Loss: 0.012207\n",
      "Epoch: 510 \tTraining Loss: 0.012210\n",
      "Epoch: 511 \tTraining Loss: 0.012214\n",
      "Epoch: 512 \tTraining Loss: 0.012217\n",
      "Epoch: 513 \tTraining Loss: 0.012221\n",
      "Epoch: 514 \tTraining Loss: 0.012224\n",
      "Epoch: 515 \tTraining Loss: 0.012227\n",
      "Epoch: 516 \tTraining Loss: 0.012230\n",
      "Epoch: 517 \tTraining Loss: 0.012233\n",
      "Epoch: 518 \tTraining Loss: 0.012235\n",
      "Epoch: 519 \tTraining Loss: 0.012238\n",
      "Epoch: 520 \tTraining Loss: 0.012241\n",
      "Epoch: 521 \tTraining Loss: 0.012243\n",
      "Epoch: 522 \tTraining Loss: 0.012245\n",
      "Epoch: 523 \tTraining Loss: 0.012247\n",
      "Epoch: 524 \tTraining Loss: 0.012249\n",
      "Epoch: 525 \tTraining Loss: 0.012251\n",
      "Epoch: 526 \tTraining Loss: 0.012253\n",
      "Epoch: 527 \tTraining Loss: 0.012255\n",
      "Epoch: 528 \tTraining Loss: 0.012257\n",
      "Epoch: 529 \tTraining Loss: 0.012258\n",
      "Epoch: 530 \tTraining Loss: 0.012260\n",
      "Epoch: 531 \tTraining Loss: 0.012262\n",
      "Epoch: 532 \tTraining Loss: 0.012263\n",
      "Epoch: 533 \tTraining Loss: 0.012264\n",
      "Epoch: 534 \tTraining Loss: 0.012266\n",
      "Epoch: 535 \tTraining Loss: 0.012267\n",
      "Epoch: 536 \tTraining Loss: 0.012268\n",
      "Epoch: 537 \tTraining Loss: 0.012270\n",
      "Epoch: 538 \tTraining Loss: 0.012271\n",
      "Epoch: 539 \tTraining Loss: 0.012272\n",
      "Epoch: 540 \tTraining Loss: 0.012273\n",
      "Epoch: 541 \tTraining Loss: 0.012274\n",
      "Epoch: 542 \tTraining Loss: 0.012275\n",
      "Epoch: 543 \tTraining Loss: 0.012276\n",
      "Epoch: 544 \tTraining Loss: 0.012277\n",
      "Epoch: 545 \tTraining Loss: 0.012278\n",
      "Epoch: 546 \tTraining Loss: 0.012278\n",
      "Epoch: 547 \tTraining Loss: 0.012279\n",
      "Epoch: 548 \tTraining Loss: 0.012280\n",
      "Epoch: 549 \tTraining Loss: 0.012281\n",
      "Epoch: 550 \tTraining Loss: 0.012282\n",
      "Epoch: 551 \tTraining Loss: 0.012282\n",
      "Epoch: 552 \tTraining Loss: 0.012283\n",
      "Epoch: 553 \tTraining Loss: 0.012284\n",
      "Epoch: 554 \tTraining Loss: 0.012284\n",
      "Epoch: 555 \tTraining Loss: 0.012285\n",
      "Epoch: 556 \tTraining Loss: 0.012285\n",
      "Epoch: 557 \tTraining Loss: 0.012286\n",
      "Epoch: 558 \tTraining Loss: 0.012287\n",
      "Epoch: 559 \tTraining Loss: 0.012287\n",
      "Epoch: 560 \tTraining Loss: 0.012288\n",
      "Epoch: 561 \tTraining Loss: 0.012288\n",
      "Epoch: 562 \tTraining Loss: 0.012289\n",
      "Epoch: 563 \tTraining Loss: 0.012289\n",
      "Epoch: 564 \tTraining Loss: 0.012290\n",
      "Epoch: 565 \tTraining Loss: 0.012290\n",
      "Epoch: 566 \tTraining Loss: 0.012291\n",
      "Epoch: 567 \tTraining Loss: 0.012291\n",
      "Epoch: 568 \tTraining Loss: 0.012292\n",
      "Epoch: 569 \tTraining Loss: 0.012292\n",
      "Epoch: 570 \tTraining Loss: 0.012292\n",
      "Epoch: 571 \tTraining Loss: 0.012293\n",
      "Epoch: 572 \tTraining Loss: 0.012293\n",
      "Epoch: 573 \tTraining Loss: 0.012294\n",
      "Epoch: 574 \tTraining Loss: 0.012294\n",
      "Epoch: 575 \tTraining Loss: 0.012294\n",
      "Epoch: 576 \tTraining Loss: 0.012295\n",
      "Epoch: 577 \tTraining Loss: 0.012295\n",
      "Epoch: 578 \tTraining Loss: 0.012296\n",
      "Epoch: 579 \tTraining Loss: 0.012296\n",
      "Epoch: 580 \tTraining Loss: 0.012296\n",
      "Epoch: 581 \tTraining Loss: 0.012297\n",
      "Epoch: 582 \tTraining Loss: 0.012297\n",
      "Epoch: 583 \tTraining Loss: 0.012297\n",
      "Epoch: 584 \tTraining Loss: 0.012298\n",
      "Epoch: 585 \tTraining Loss: 0.012298\n",
      "Epoch: 586 \tTraining Loss: 0.012298\n",
      "Epoch: 587 \tTraining Loss: 0.012299\n",
      "Epoch: 588 \tTraining Loss: 0.012299\n",
      "Epoch: 589 \tTraining Loss: 0.012299\n",
      "Epoch: 590 \tTraining Loss: 0.012300\n",
      "Epoch: 591 \tTraining Loss: 0.012300\n",
      "Epoch: 592 \tTraining Loss: 0.012300\n",
      "Epoch: 593 \tTraining Loss: 0.012301\n",
      "Epoch: 594 \tTraining Loss: 0.012301\n",
      "Epoch: 595 \tTraining Loss: 0.012301\n",
      "Epoch: 596 \tTraining Loss: 0.012301\n",
      "Epoch: 597 \tTraining Loss: 0.012302\n",
      "Epoch: 598 \tTraining Loss: 0.012302\n",
      "Epoch: 599 \tTraining Loss: 0.012302\n",
      "Epoch: 600 \tTraining Loss: 0.012303\n",
      "Finished training\n"
     ]
    }
   ],
   "source": [
    "h_dim = 8 # Number of neurons in hidden layer\n",
    "\n",
    "model = SRN(1, h_dim) # The configuration of SRN is in:hn-cu:out (1:44:1)\n",
    "\n",
    "print(model)\n",
    "\n",
    "w = w.float().requires_grad_(True)\n",
    "l = l.float().requires_grad_(True)\n",
    "\n",
    "epochs = 600\n",
    "\n",
    "optim = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
    "\n",
    "bce = torch.nn.BCELoss()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    rl = 0.0\n",
    "    i = 0\n",
    "    for f, t in samples:\n",
    "        optim.zero_grad()\n",
    "        \n",
    "        p = model(f)\n",
    "        \n",
    "        loss = bce(p, t)\n",
    "        \n",
    "        loss.backward()\n",
    "\n",
    "        optim.step()\n",
    "        \n",
    "        rl += loss.item()\n",
    "    \n",
    "    # Handcraft learnnig rate decay\n",
    "    if epoch % (0.2*epochs) == 0 and epoch > 0:\n",
    "        for param_group in optim.param_groups:\n",
    "            print(f\"Changing learning rate to {param_group['lr']/2}\")\n",
    "            param_group['lr'] = param_group['lr']/2\n",
    "    \n",
    "    rl /= n_samples\n",
    "    \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f}'.format(epoch+1, rl))\n",
    "        \n",
    "print(\"Finished training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference/Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc:1.0\n"
     ]
    }
   ],
   "source": [
    "# Input test sequence\n",
    "wtest = torch.randint(0, 2, (100, 1,))\n",
    "\n",
    "# output test sequence\n",
    "ltest = xor(wtest)\n",
    "\n",
    "test_samples = list(zip(wtest.float(), ltest.float()))\n",
    "c = 0\n",
    "for f, t in test_samples:\n",
    "    w = round(f.item())\n",
    "    y = round(model(f).item())\n",
    "    t = round(t.item())\n",
    "    \n",
    "    #print(f\"w: {w}, y:{y}, t:{t}\")\n",
    "    \n",
    "    if y == t:\n",
    "        c += 1\n",
    "\n",
    "print(\"acc:\", end= '')        \n",
    "print(c/len(wtest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like some learning has happened.... but I stopped here because the purpose of this task was just to understand recurrence in a simple example using an SRN. \n",
    " \n",
    "The backpropagation in this example is based on experiments presented by Jeff Elman [3] and is called truncated backpropagation. It basically means that $s_j(t − 1)$ is simply regarded as an additional input. Any error at the hidden (state) layer, is used to modify weights from this additional input slot.\n",
    "\n",
    "Although it is not optimal, it is used for learning purposes. To overcome this simplification, errors can be backpropagated even further. This is called backpropagation through time (BPTT) and is an extension of what we have done so far. The basic principle of BPTT is “unfolding”. All recurrent weights can be duplicated spatially for an arbitrary number of time steps, usually referred to as $\\tau$, in our case $\\tau=1$.\n",
    "\n",
    "More complex models can be build using recurrent layers such as [nn.RNN, nn.LSTM, nn.GRU](https://pytorch.org/docs/stable/nn.html) from pytorch or other frameworks with buit-in BPTT.\n",
    "\n",
    "The next steps will focus on understanding the fundamentals of transformer networks with simple examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference:\n",
    "1. https://www.youtube.com/watch?v=ySEx_Bqxvvo&list=PLtBw6njQRU-rwp5__7C0oIVt26ZgjG9NI&index=2\n",
    "2. https://github.com/ramonlins/obsidian/tree/feature/dl/rnn/Papers/recurrent%20networks/Tomas%20Mikolov\n",
    "3. https://github.com/ramonlins/obsidian/blob/feature/dl/rnn/Papers/recurrent%20networks/Tomas%20Mikolov/references/elman1990.pdf\n",
    "\n",
    "Additional Materials:\n",
    "\n",
    "https://pabloinsente.github.io/the-recurrent-net"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
